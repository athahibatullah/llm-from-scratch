{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO2gWRIQU8nkLVR1aPA6yPq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/athahibatullah/llm-from-scratch/blob/main/Pytorch_Appendix_A.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **A.1 What is Pytorch?**"
      ],
      "metadata": {
        "id": "VFeylDMwpgPw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **A.1.1 The Three Core Components of PyTorch**"
      ],
      "metadata": {
        "id": "zG-ROj02vgae"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "PyTorch is an open source Python-based deep learning library. Pytorch has been the most widely used deep learning library for research since 2019 by a wide margin.\n",
        "\n",
        "One of the reasons PyTorch is so popular is its user-friendly interface and efficiency. Despite its accessibility, it doesn't compromise on flexibility, allowing advanced users to tweak lower-level aspects of their models for customization and optimization. In short, for many practitioners and researchers, PyTorch offers just the right balance between usability and features.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1CwRGaZ3_oEKUdxZbwXZiK86Jbppfi6ag\">\n",
        "\n",
        "* Tensor Library: PyTorch is a tensor library that extends the concept of the array-oriented programming library NumPy with the additional feature that accelerates computation on GPUs, thus providing a seamless switch between CPUs and GPUs.\n",
        "\n",
        "* Automatic Differentiation Engine: PyTorch is an automatic differentiation engine, also known as autograd, that enables the automatic computation of gradients for tensor operations, simplifying backpropagation and model optimization.\n",
        "\n",
        "* Deep Learning Library: PyTorch is a deep learning library. It offers modular, flexible, and efficient building blocks, including pretrained models, loss functions, and optimizers, for designing and training a wide range of deep learning models, catering to both researchers and developers."
      ],
      "metadata": {
        "id": "IU4CHCX8pmKu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **A.2 Understanding Tensors**"
      ],
      "metadata": {
        "id": "pRmJq8K3vpw_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tensors represent a mathematical concept that generalizes vector and matrices to potentially higher dimensions. Tensors are mathematical object that can be characterized by their order (or rank), which provides the number of dimensions. For example, scalar is tensor rank 0, vector is tensor rank 1, and matrices is tensor rank 2.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1Gn7mKtrvucwRRZxmq2x69tF-ygOAK5iL\">\n",
        "\n"
      ],
      "metadata": {
        "id": "UBI2zqA2w9Dv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From a computational perspective, tensors serve as data containers. They hold multidimensional data, where each dimension represent different feature. Tensor library like PyTorch can create, manipulate, and compute with these arrays efficiently. In this context, a tensor library functions as an array library.\n",
        "\n",
        "PyTorch tensors are similar to NumPy arrays but have several additional features that are important for deep learning. For example, automatic differentiation engine, simplifying computing gradients. PyTorch also support GPU to speed up deep neural network training process."
      ],
      "metadata": {
        "id": "Duz8EC4gzFsx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **A.2.1 Scalars, Vectors, Matrices, and Tensors**"
      ],
      "metadata": {
        "id": "N63JEmQyw2-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As mentioned earlier, PyTorch tensors are data containers for array-like structures. Scalar is zero-dimensional tensors (0D tensor), Vectors is one-dimensional tensors (1D tensor), Matrix is two-dimensional tensors (2D tensor). For three-dimensional and above, we just call it 3D tensor and so forth. We can create objects of PyTorch's Tensor class using the torch.tensor function as shown:"
      ],
      "metadata": {
        "id": "P3oK3jyd1zUR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "print(torch.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p45fRcAc2b4i",
        "outputId": "9fcdcd44-6f7e-4e16-b227-671e2a76f4cd"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.6.0+cu124\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(torch.cuda.is_available())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ErmiuM342j9R",
        "outputId": "dde53097-6249-47ab-e5a5-7549224945f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create a 0D tensor (scalar) from a Python integer\n",
        "tensor0d = torch.tensor(1)\n",
        "\n",
        "# create a 1D tensor (vector) from a Python list\n",
        "tensor1d = torch.tensor([1, 2, 3])\n",
        "\n",
        "# create a 2D tensor from a nested Python list\n",
        "tensor2d = torch.tensor([[1, 2],\n",
        "                         [3, 4]])\n",
        "\n",
        "# create a 3D tensor from a nested Python list\n",
        "tensor3d_1 = torch.tensor([[[1, 2], [3, 4]],\n",
        "                           [[5, 6], [7, 8]]])\n",
        "\n",
        "# create a 3D tensor from NumPy array\n",
        "ary3d = np.array([[[1, 2], [3, 4]],\n",
        "                  [[5, 6], [7, 8]]])\n",
        "tensor3d_2 = torch.tensor(ary3d)  # Copies NumPy array\n",
        "tensor3d_3 = torch.from_numpy(ary3d)  # Shares memory with NumPy array"
      ],
      "metadata": {
        "id": "ZzUWUkRE2zTN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ary3d[0, 0, 0] = 999\n",
        "print(tensor3d_2) # remains unchanged"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ba-UQrUg23pr",
        "outputId": "e81120b2-ad83-4e4a-8e25-21e1801719a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[1, 2],\n",
            "         [3, 4]],\n",
            "\n",
            "        [[5, 6],\n",
            "         [7, 8]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tensor3d_3) # changes because of memory sharing"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5iODrRIc24cD",
        "outputId": "38f743fa-3ec6-4301-ed39-875f9e515478"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[999,   2],\n",
            "         [  3,   4]],\n",
            "\n",
            "        [[  5,   6],\n",
            "         [  7,   8]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **A.2.2 Tensor Data Types**"
      ],
      "metadata": {
        "id": "FaEI3ne_3Fw6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "PyTorch adopts the default 64-bit integer data type from Python. We can access the data type of a tensor via the .dtype attribute of a tensor"
      ],
      "metadata": {
        "id": "t3hF-XCu3TrS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tensor1d = torch.tensor([1, 2, 3])\n",
        "print(tensor1d.dtype)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Q2Jfk6n25uS",
        "outputId": "15dab18b-614e-4cc2-fcdd-a17225a22d2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we create tensors from float, by default PyTorch creates 32-bit precision"
      ],
      "metadata": {
        "id": "JrVWi7dr3f0i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "floatvec = torch.tensor([1.0, 2.0, 3.0])\n",
        "print(floatvec.dtype)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qaOEZjoq3fOE",
        "outputId": "f4348f36-e2f7-49d8-e298-7d07b9aaf122"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.float32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is primarily due 32-bit is sufficent for most deep learning tasks while consuming less memory and computational resoureces than 64-bit. Moreover, GPU architectures are optimized for 32-bit computations, and using this type of data can significantly speed up model training and inference."
      ],
      "metadata": {
        "id": "jBUF4Lmf3-FS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It's also possible to conver 64-bit into 32-bit"
      ],
      "metadata": {
        "id": "Cu9IHNzg4S5i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "floatvec = tensor1d.to(torch.float32)\n",
        "print(floatvec.dtype)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KeJclBhY3kZL",
        "outputId": "019bc3e7-f122-4e6c-8344-78d985b8d179"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.float32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **A.2.3 Common PyTorch Tensor Operations**"
      ],
      "metadata": {
        "id": "bLj9X4jM4Za8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is our code to generate tensors from earlier:"
      ],
      "metadata": {
        "id": "-G5RlSNn44G9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tensor2d = torch.tensor([[1, 2, 3],\n",
        "                         [4, 5, 6]])\n",
        "tensor2d"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "huz_vXTO4gdq",
        "outputId": "2f200a6b-06a0-4f07-f0b5-e58a5aaaf3a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1, 2, 3],\n",
              "        [4, 5, 6]])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ".shape is used to access the shape of tensor"
      ],
      "metadata": {
        "id": "88Ctd1Se5B-c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tensor2d.shape # row,column"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Wa4T84B4lBT",
        "outputId": "7513e8cd-1d62-450c-b764-28ff70f13877"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 3])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also reshape the tensor from 2x3 to 3x2"
      ],
      "metadata": {
        "id": "siID-0w85Gs8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tensor2d.reshape(3, 2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "puupc2eB4mT9",
        "outputId": "3e48d05e-dfea-48f8-fd1c-6a922a1957ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1, 2],\n",
              "        [3, 4],\n",
              "        [5, 6]])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "However, .view is more common for reshaping tensor"
      ],
      "metadata": {
        "id": "-JX_FffS5fUc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tensor2d.view(3, 2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "81NWOJE24oIC",
        "outputId": "60173754-65f5-4006-f80d-3444c7770351"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1, 2],\n",
              "        [3, 4],\n",
              "        [5, 6]])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Similar to .reshape and .view, PyTorch also offers many syntax with the same operations & computations. This is because initially PyTorch followed the original Torch syntax convention but then, by popular request, added syntax to make it similar to NumPy."
      ],
      "metadata": {
        "id": "eSAzTl0b5sVt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we have .T to transpose a tensor"
      ],
      "metadata": {
        "id": "n108wMWE6Is1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tensor2d.T"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IFupxebm4pQc",
        "outputId": "b6044877-94ce-4d1e-8ba1-e57082861a3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1, 4],\n",
              "        [2, 5],\n",
              "        [3, 6]])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ".matmul is used to multiply two matrices in PyTorch"
      ],
      "metadata": {
        "id": "YmGEGfg56QAt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tensor2d.matmul(tensor2d.T)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RZpfqY-V4qUj",
        "outputId": "abe91dcb-748c-47ab-b00b-f5f1cf170827"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[14, 32],\n",
              "        [32, 77]])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "we can also use @ operator to do the same as .matmul"
      ],
      "metadata": {
        "id": "uTOyfyBh6cz-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tensor2d @ tensor2d.T"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J5VV6sul4trm",
        "outputId": "fb9a5e2c-7e94-47fc-bec4-95a1f37409e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[14, 32],\n",
              "        [32, 77]])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **A.3 Seeing Models as Computation Graphs**"
      ],
      "metadata": {
        "id": "UN9-pmlG6jse"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's look at PyTorch's automatic differentiation engine, also known as autograd. Pytorch's autograd system provides functions to compute gradients in dynamic computational graphs automatically.\n",
        "\n",
        "A computational graph is a directed graph that allows us to express and visualize mathematical expressions. In deep learning, a computation graph lays out the sequence of calculations needed to compute the output of a neural network. We will need this to compute the required gradients for backpropagation, the main training algorithm for neural networks.\n",
        "\n",
        "Let's look at a concrete example to illustrate the concept of a computation graph. The code in the following listing implements the forward pass (prediction step) of a simple logistic regression classifier, which can be seen as a single-layer neural network. It returns a score between 0 and 1, which is compared to the true class label (0 or 1) when computing the loss."
      ],
      "metadata": {
        "id": "sHzYX5tO6wAp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F # This import statement is a common convention in PyTorch to prevent long lines of code\n",
        "\n",
        "y = torch.tensor([1.0])  # true label\n",
        "x1 = torch.tensor([1.1]) # input feature\n",
        "w1 = torch.tensor([2.2]) # weight parameter\n",
        "b = torch.tensor([0.0])  # bias unit\n",
        "\n",
        "z = x1 * w1 + b          # net input\n",
        "a = torch.sigmoid(z)     # activation & output\n",
        "\n",
        "loss = F.binary_cross_entropy(a, y)\n",
        "print(loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g4XbHUGV6iqX",
        "outputId": "8c6e3bdc-30ad-4093-e8ec-11adbed8064a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.0852)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Figure below illustrates the concept of computation graph"
      ],
      "metadata": {
        "id": "Ai1lGrX37osv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://drive.google.com/uc?id=1TxVSmxeVfWNPiksT0XNhglafj_xlEBfS\">"
      ],
      "metadata": {
        "id": "h8zG2aQt-qps"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "PyTorch builds such a computation graph in the background, and we can use this to calculate gradients of a loss function with respect to the model parameters (here w1 and b) to train the model."
      ],
      "metadata": {
        "id": "4DHEihHJ_s9R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **A.4 Automatic Differentiation Made Easy**"
      ],
      "metadata": {
        "id": "BNIjSTm8opKM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can build a computational graph internally by default if one of its terminal nodes has the requires_grad attribute set to True. This is useful if we want to compute gradients. Gradients are required when training neural networks via the popular backpropagation algorithm.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1FBYR12vad2hwlzIYKT1eDMIdATZV2z3l\">\n",
        "\n",
        "Figure above shows partial derivatives, which measure the rate at which a function changes with respect to one of its variables. A gradient is a vector containing all of the partial derivatives of a multivariate function, a function with more than one variable as input.\n",
        "\n",
        "On a high level, all you need to know for this book is that the chain rule is a way to compute gradients of a loss function given the model's parameters in a computation graph. This provides the information needed to update each parameter to minimize the loss function, which serves as a proxy for measuring the model's performance using a method such as gradient descent.\n",
        "\n",
        "This all related to automatic differentiation (autograd) (PyTorch's second component). PyTorch's autograd engine constructs a computational graph in the background by tracking every operation performed on tensors. Then by calling the grad function, we can compute the gradient of the loss concerning the model parameter w1.\n"
      ],
      "metadata": {
        "id": "5Dw68cUkpKVb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "from torch.autograd import grad\n",
        "\n",
        "y = torch.tensor([1.0])\n",
        "x1 = torch.tensor([1.1])\n",
        "w1 = torch.tensor([2.2], requires_grad=True)\n",
        "b = torch.tensor([0.0], requires_grad=True)\n",
        "\n",
        "z = x1 * w1 + b\n",
        "a = torch.sigmoid(z)\n",
        "\n",
        "loss = F.binary_cross_entropy(a, y)\n",
        "\n",
        "grad_L_w1 = grad(loss, w1, retain_graph=True)\n",
        "grad_L_b = grad(loss, b, retain_graph=True)\n",
        "\n",
        "print(grad_L_w1)\n",
        "print(grad_L_b)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FU6G3gM2vQlC",
        "outputId": "94e22884-4a97-4ac8-a7a7-2a0d5513e023"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(tensor([-0.0898]),)\n",
            "(tensor([-0.0817]),)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have been using the grad function manually, which can be useful for experimentation, debugging, and demonstrating concepts. But in practice, PyTorch provides even more high-level tools to automate this process. For example, by calling .backward on the loss, PyTorch will compute the gradients of all the leaf nodes in the graph, which will be stored via the tensor's .grad attributes"
      ],
      "metadata": {
        "id": "C6rZpJo4vs7V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss.backward()\n",
        "\n",
        "print(w1.grad)\n",
        "print(b.grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i6Vwmk4Zvcol",
        "outputId": "765a69a0-729e-49d8-9b70-d70fd1106405"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([-0.0898])\n",
            "tensor([-0.0817])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "WTidayHUvixj"
      }
    }
  ]
}