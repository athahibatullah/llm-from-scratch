{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1dk2SsP7tBK5lhS6vcwdCrmAVvpMr0QMs",
      "authorship_tag": "ABX9TyP0CKkbw2UdD91BJUJntH35",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/athahibatullah/llm-from-scratch/blob/main/02%20-%20Working%20with%20Text%20Data/ch02.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2.1 Understanding Word Embeddings**"
      ],
      "metadata": {
        "id": "9qeAgiAsbNAo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Deep neural networks including LLMs cannot process raw text directly since text is categorical. Categorical mean it isn't compatible with the mathematical operations used to implement and train neural networks. Therefore, we need a way to represent words as continuous-valued vectors. The concept of converting data into a vector format is often referred to as embedding. Different data formats require distinct embedding models, for example an embedding model designed for text would not be suitable for embedding audio or video data.\n",
        "<img src=\"https://drive.google.com/uc?id=1Uu1CZJM8mGLImwADawl9CCB0xCicfocF\">\n",
        "\n",
        "Embedding is a mapping from discrete objects, such as words, images, or even entire documents, to points in a continuous vector space. The primary purpose of embeddings is to convert nonnumeric data into a format that neural networks can process. Text embedding is not limited to word embeddings, there are also embeddings for sentence, paragraphs, or whole documents. Sentence or paragraph embeddings are popular choices for Retrieval-Augmented Generation (RAG). Retrieval-Augmented Generation combines generation (like producing text) with retrieval (like searching an external knowledge base) to pull relevant information when generating text, RAG isn't included in this scope of building LLM from scratch. We will focus on word embeddings.\n",
        "\n",
        "One of the earlier and most popular examples of word embeddings algorithm is Word2Vec. Word2Vec trained neural network architecture to generate word embeddings by predicting the context of a word given the target word or vice versa. The main idea of Word2Vec is word that appear in similar contexts tend to have similar meanings. This mean if we project them into two-dimensional word embeddings for visualization purpose, similar terms are clustered together. Word embeddings can have varying dimensions, from one to thousands. More dimension is better but with more computational cost.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=16PcjL0A-UuN9O3CxZ4cjgGh2DtjEZ_nl\">\n",
        "\n",
        "While we can use pretrained models such as Word2Vec to generate embeddings for machine learning models, LLMs commonly produce their own embeddings that are part of the input layer and are updated during training. The advantage of optimizing the embeddings as part of the LLM training instead of using Word2Vec is that the embeddings are optimized to the specific task and data at hand.\n",
        "\n",
        "High-dimensional embeddings present a challenge for visualization because humans are limited to seeing a 3 or fewer dimensions. When working with LLMs, we typically use embedding with a much higher dimensionality. The smallest GPT-2 models (117M and 125M paramters) use an embedding size of 768 dimensions and the largest GPT-3 model (175B parameters) use an embedding size of 12288 dimensions."
      ],
      "metadata": {
        "id": "1CkfUGP0axmN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2.2 Tokenizing Text**"
      ],
      "metadata": {
        "id": "xm0GDaQBSjqz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's split input text into individual tokens first as it's required preprocessing step for creating embeddings for an LLM.\n",
        "<img src=\"https://drive.google.com/uc?id=1rkJu1MKBaQbVSEETK07rxCulowe1COZQ\">\n",
        "\n",
        "The text we will tokenize for LLM training is \"The Verdict\", a short story by Edith Wharton. The text is available on Wikisource: https://en.wikisource.org/wiki/The_Verdict."
      ],
      "metadata": {
        "id": "KSWVv-L5cOoq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, we will load the text file: the-verdict.txt, count total number of character and print the first 100 character of the file."
      ],
      "metadata": {
        "id": "iq7AxEYrcthW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7vv4aIxnOZ5R",
        "outputId": "18792951-389e-4fbd-c804-03641684b132"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "10y9XsSOBQZO"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import urllib.request\n",
        "\n",
        "if not os.path.exists(\"the-verdict.txt\"):\n",
        "    url = (\"https://raw.githubusercontent.com/rasbt/\"\n",
        "           \"LLMs-from-scratch/main/ch02/01_main-chapter-code/\"\n",
        "           \"the-verdict.txt\")\n",
        "    file_path = \"the-verdict.txt\"\n",
        "    urllib.request.urlretrieve(url, file_path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    raw_text = f.read()\n",
        "\n",
        "print(\"Total number of character:\", len(raw_text))\n",
        "print(raw_text[:99])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qXHatRMcBklR",
        "outputId": "a65ac4c8-f2b8-4a3d-a290-72e53f107f11"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of character: 20479\n",
            "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can start splitting the text by using a short sentence first to make sure our splitting function works as intended."
      ],
      "metadata": {
        "id": "gvuz9n_AcyMC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "text = \"Hello, world. This, is a test.\"\n",
        "result = re.split(r'(\\s)', text)\n",
        "\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yY-BXAZCBsp0",
        "outputId": "047e8023-cb88-4b08-a286-1df8eae0ae3f"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello,', ' ', 'world.', ' ', 'This,', ' ', 'is', ' ', 'a', ' ', 'test.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = re.split(r'([,.]|\\s)', text)\n",
        "\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HnsG0KS_BtRi",
        "outputId": "2a826e03-a9e5-406e-fac7-9e3a20c84e05"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', ',', '', ' ', 'world', '.', '', ' ', 'This', ',', '', ' ', 'is', ' ', 'a', ' ', 'test', '.', '']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Strip whitespace from each item and then filter out any empty strings.\n",
        "result = [item for item in result if item.strip()]\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jd2NNyWwBupj",
        "outputId": "90ab3f03-4ca1-4359-ad00-48501db25b9b"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', ',', 'world', '.', 'This', ',', 'is', 'a', 'test', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Handle punctuations\n",
        "text = \"Hello, world. Is this-- a test?\"\n",
        "\n",
        "result = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
        "result = [item.strip() for item in result if item.strip()]\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sYG3GZ0GBxXy",
        "outputId": "c8cd193b-c9d0-473d-97c2-09ef0e64d5f7"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', ',', 'world', '.', 'Is', 'this', '--', 'a', 'test', '?']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After successfully create splitting function, we can apply it first to the previous first 100 characters and after that apply it to the entire document.\n",
        "\n",
        "Note that we refrain from making all text lower-case because capitalization helps LLMs distinguish between proper nouns and common nouns, understand sentence structure, and learn to generate text with proper capitalization.\n",
        "\n",
        "When developing a simple tokenizer, it's up to us to decide whether we should encode whitespaces as separate characters or just remove them as it's depends on our application and its requirements. Removing whitespaces reduces the memory and computing requirements, but can also be helpful if we train models that are sensitive to the exact structure of the text like for example python code as it's sensitive to indentation and spacing. In this example, we remove whitespaces for simplicity and brevity of the tokenized outputs. Later, we will switch to a tokenization scheme that includes whitespaces."
      ],
      "metadata": {
        "id": "heICztE8c8CV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
        "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
        "print(preprocessed[:30])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "of6V2tSSBy7r",
        "outputId": "6f15aae5-3c96-429f-d904-91dc88b8e6b5"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(preprocessed))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "le4k4waYB1Qm",
        "outputId": "362f3fa1-8235-4972-ecfd-ce5cf27555ba"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4690\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2.3 Converting Tokens into Token IDs**"
      ],
      "metadata": {
        "id": "z_BD6OT-SzyY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After successfully extracting token from the text, we convert these tokens to an integer representation to produce the token IDs. To map the previously generated tokens into token IDs, we have to build a vocabulary first. This vocabulary defines how we map each unique word and special character to a unique integer.\n",
        "<img src=\"https://drive.google.com/uc?id=1DgFbB-9X8qR9s_M13UUyuIhh8kyB2yPg\">\n",
        "\n",
        "Unique tokens mean duplicated word/punctuation will be counted as 1."
      ],
      "metadata": {
        "id": "dHpThs2Td2gp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_words = sorted(set(preprocessed))\n",
        "vocab_size = len(all_words)\n",
        "\n",
        "print(vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yxoQX1eAS40y",
        "outputId": "d8dddfb4-325c-4fa1-f16b-2bb2a1c37679"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1130\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = {token:integer for integer,token in enumerate(all_words)}"
      ],
      "metadata": {
        "id": "RN2RLaeaS84U"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, item in enumerate(vocab.items()):\n",
        "    print(item)\n",
        "    if i >= 50:\n",
        "        break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vQ9pHeywS-hK",
        "outputId": "e435c387-ccb6-4c7c-dcc5-7a310d343bed"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('!', 0)\n",
            "('\"', 1)\n",
            "(\"'\", 2)\n",
            "('(', 3)\n",
            "(')', 4)\n",
            "(',', 5)\n",
            "('--', 6)\n",
            "('.', 7)\n",
            "(':', 8)\n",
            "(';', 9)\n",
            "('?', 10)\n",
            "('A', 11)\n",
            "('Ah', 12)\n",
            "('Among', 13)\n",
            "('And', 14)\n",
            "('Are', 15)\n",
            "('Arrt', 16)\n",
            "('As', 17)\n",
            "('At', 18)\n",
            "('Be', 19)\n",
            "('Begin', 20)\n",
            "('Burlington', 21)\n",
            "('But', 22)\n",
            "('By', 23)\n",
            "('Carlo', 24)\n",
            "('Chicago', 25)\n",
            "('Claude', 26)\n",
            "('Come', 27)\n",
            "('Croft', 28)\n",
            "('Destroyed', 29)\n",
            "('Devonshire', 30)\n",
            "('Don', 31)\n",
            "('Dubarry', 32)\n",
            "('Emperors', 33)\n",
            "('Florence', 34)\n",
            "('For', 35)\n",
            "('Gallery', 36)\n",
            "('Gideon', 37)\n",
            "('Gisburn', 38)\n",
            "('Gisburns', 39)\n",
            "('Grafton', 40)\n",
            "('Greek', 41)\n",
            "('Grindle', 42)\n",
            "('Grindles', 43)\n",
            "('HAD', 44)\n",
            "('Had', 45)\n",
            "('Hang', 46)\n",
            "('Has', 47)\n",
            "('He', 48)\n",
            "('Her', 49)\n",
            "('Hermia', 50)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "set() function will return a list of unique token, and sorted() will sort the list. We will only print the first 50 vocabulary.\n",
        "\n",
        "Next, our goal is to apply this vocabulary to convert our text into token IDs.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=15xDpA-gvfAAghdZA06hLV6wGeLnG0gKr\">\n"
      ],
      "metadata": {
        "id": "K_ClEhITeIXD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleTokenizerV1:\n",
        "    def __init__(self, vocab):\n",
        "        self.str_to_int = vocab # Stores the vocabulary as a class attribute for access in the encode and decode methods\n",
        "        self.int_to_str = {i:s for s,i in vocab.items()} # Creates an inverse vocabulary that maps\n",
        "                                                         # token into IDs back to the original text tokens\n",
        "\n",
        "    def encode(self, text): # Processes input text into token IDs\n",
        "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
        "\n",
        "        preprocessed = [\n",
        "            item.strip() for item in preprocessed if item.strip()\n",
        "        ]\n",
        "        ids = [self.str_to_int[s] for s in preprocessed]\n",
        "        return ids\n",
        "\n",
        "    def decode(self, ids): # Converts token IDs back into text\n",
        "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
        "        # Replace spaces before the specified punctuations\n",
        "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
        "        return text"
      ],
      "metadata": {
        "id": "k1JRsKA5TAFZ"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We create an inverse vocabulary that maps token IDs back to the original text tokens, an encode to process input text into token IDs, and decode to converts token IDs back into text."
      ],
      "metadata": {
        "id": "uej7vaUpeggN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = SimpleTokenizerV1(vocab)\n",
        "\n",
        "text = \"\"\"\"It's the last he painted, you know,\"\n",
        "           Mrs. Gisburn said with pardonable pride.\"\"\"\n",
        "ids = tokenizer.encode(text)\n",
        "print(ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q8kczEyCTFAw",
        "outputId": "d601af2b-5cda-45eb-ceef-cc6983bfafc9"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 56, 2, 850, 988, 602, 533, 746, 5, 1126, 596, 5, 1, 67, 7, 38, 851, 1108, 754, 793, 7]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We try our encode function and it's working. Next let's decode to see our text back."
      ],
      "metadata": {
        "id": "BXy6Lyh7ejBo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "vZVyY9l0TGdy",
        "outputId": "63908858-606c-4942-ab94-206c8bfc37b0"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\" It\\' s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(tokenizer.encode(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "xo8qcUf1TH9w",
        "outputId": "a54bc43e-fe21-4d4b-d061-9db1b90cb99f"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\" It\\' s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It successfully decoded our text into a readable human text."
      ],
      "metadata": {
        "id": "kBdEpefxer0E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2.4 Adding Special Context Tokens**"
      ],
      "metadata": {
        "id": "_ELMfjsyx7fQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need to modify the tokenizer to handle unknown words. We also need to address the usage and addition of special context tokens that can enhance a model's understanding of context or other relevant information in the text. These special tokens can include markers for unknown words and document boundaries. For example, we will modify the vocabulary and add special token to handle unknown words and end of text. End of text is needed to separate different dataset since LLM need to be trained using multiple books, articles, documents, or even separating different context in the same documents and this is where this special token come into play. For unknown words (words doesn't exist in current vocabulary), we name <|unk|>. For end of text, we name <|endoftext|>.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1QVadUQfaqZQm79uk32iS1o4wLT_T6zO6\" >\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1RHETuKRD5Zr89fH7nHP3lgS-QY77SrYG\" >\n",
        "\n",
        "To better understand, lets encode a new text with word that doesn't exist in current vocabulary"
      ],
      "metadata": {
        "id": "KVqnyf5SiTle"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = SimpleTokenizerV1(vocab)\n",
        "\n",
        "text = \"Hello, do you like tea. Is this-- a test?\"\n",
        "\n",
        "tokenizer.encode(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "WSpUY3fNyBnl",
        "outputId": "2ba7bd79-d2c7-46f9-ea96-b1157073b310"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'Hello'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-64-2162118319>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Hello, do you like tea. Is this-- a test?\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-33-323612697>\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0mitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpreprocessed\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         ]\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr_to_int\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpreprocessed\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-33-323612697>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0mitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpreprocessed\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         ]\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr_to_int\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpreprocessed\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'Hello'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It will return error since \"Hello\" token isn't registered in vocabulary. Now let's modify the code and add <|unk|> and <|endoftext|>."
      ],
      "metadata": {
        "id": "X8mP-xegivoL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_tokens = sorted(list(set(preprocessed)))\n",
        "all_tokens.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
        "\n",
        "vocab = {token:integer for integer,token in enumerate(all_tokens)}"
      ],
      "metadata": {
        "id": "J0x_xzWqyj9w"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(vocab.items())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qwRf8ypCymqI",
        "outputId": "aebab7c4-c1ac-40d9-f28c-a4446e561e13"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1132"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i, item in enumerate(list(vocab.items())[-5:]):\n",
        "    print(item)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lnPJQ6DKyqGw",
        "outputId": "b4e5afc0-5a5d-4374-a682-14583d21e1cc"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('younger', 1127)\n",
            "('your', 1128)\n",
            "('yourself', 1129)\n",
            "('<|endoftext|>', 1130)\n",
            "('<|unk|>', 1131)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we successfully added our new token. After adding new token, we should modify our encode & decode function to use this new token.\n",
        "\n"
      ],
      "metadata": {
        "id": "N79yxGbDiy9_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleTokenizerV2:\n",
        "    def __init__(self, vocab):\n",
        "        self.str_to_int = vocab\n",
        "        self.int_to_str = { i:s for s,i in vocab.items()}\n",
        "\n",
        "    def encode(self, text):\n",
        "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
        "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
        "        preprocessed = [ # Replaces unknown words by <|unk|>\n",
        "            item if item in self.str_to_int\n",
        "            else \"<|unk|>\" for item in preprocessed\n",
        "        ]\n",
        "\n",
        "        ids = [self.str_to_int[s] for s in preprocessed]\n",
        "        return ids\n",
        "\n",
        "    def decode(self, ids):\n",
        "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
        "        # Replace spaces before the specified punctuations\n",
        "        text = re.sub(r'\\s+([,.:;?!\"()\\'])', r'\\1', text) # Replaces spaces before the specified\n",
        "        return text"
      ],
      "metadata": {
        "id": "gQt8LLw8yrxg"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's test our modified function."
      ],
      "metadata": {
        "id": "U-FNUPTDi2f7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = SimpleTokenizerV2(vocab)\n",
        "\n",
        "text1 = \"Hello, do you like tea?\"\n",
        "text2 = \"In the sunlit terraces of the palace.\"\n",
        "\n",
        "text = \" <|endoftext|> \".join((text1, text2))\n",
        "\n",
        "print(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w_kAN_IUytMn",
        "outputId": "c109908e-c2ce-4b64-cc07-64a7fdcb8579"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello, do you like tea? <|endoftext|> In the sunlit terraces of the palace.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.encode(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZCGyi7U2yuon",
        "outputId": "823ebf4e-c733-4557-c807-2b00de5f705c"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1131, 5, 355, 1126, 628, 975, 10, 1130, 55, 988, 956, 984, 722, 988, 1131, 7]"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(tokenizer.encode(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "uLoj4li0yvYp",
        "outputId": "1845ed9b-50e2-459f-b807-e3e357e412c2"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<|unk|>, do you like tea? <|endoftext|> In the sunlit terraces of the <|unk|>.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It's now added our new special tokens.\n",
        "\n",
        "Depending on the LLM, some researchers also consider additional special tokens such as the following:\n",
        "\n",
        "[BOS] (beginning of sequence): This token marks the start of a text. It signifies to the LLM where a piece of content begins.\n",
        "\n",
        "[EOS] (end of sequence): This token is positioned at the end of a text and is especially useful when concatenating multiple unrelated texts, similar to <|endoftext|>. For instance, when combining two different Wikipedia articles or books, the [EOS] indicates where one ends and the next begins.\n",
        "\n",
        "[PAD] (padding): When training LLMs with batch sizes larger than one, the batch might contain texts of varying lengths. To ensure all texts have the same length, the shorter texts are extended or \"padded\" using the [PAD] token, up to the length of the longest text in the batch.\n",
        "The tokenizer used for GPT models does not need any of these tokens. <|endoftext|> is enough for simplicity. <|endoftext|> is analogous to the [EOS] and can also be used for padding. On later chapter when training on batched inputs introduced, we typically use a mask, meaning we don't attend to padded tokens. Morover, the tokenized used for GPT models also doesn't use <|unk|> for unknown words but instead use a byte pair encoding tokenizer, which breaks words down into subword units, which we will discuss next."
      ],
      "metadata": {
        "id": "DTiYQ7vzi6Zd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2.5 Byte Pair Encoding**"
      ],
      "metadata": {
        "id": "bxha2STBUFhf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Byte Pair Encoding (BPE) was used to train LLM like GPT-2, GPT-3, and the original model used in ChatGPT. We will use library called tiktoken, which implements the BPE algorithm very efficiently based on source code in Rust.\n",
        "\n",
        "Let's install the tiktoken first."
      ],
      "metadata": {
        "id": "_lz-MKCrjHjm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tiktoken"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y-lEAYPsUJAP",
        "outputId": "d1b1ffa4-460c-432c-a965-119431559a59"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (0.9.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2025.4.26)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import importlib\n",
        "import tiktoken\n",
        "\n",
        "print(\"tiktoken version:\", importlib.metadata.version(\"tiktoken\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "forgBnxyUPnX",
        "outputId": "dbbf5c33-2a27-4335-9741-bc5fadc6c5a6"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tiktoken version: 0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = tiktoken.get_encoding(\"gpt2\")"
      ],
      "metadata": {
        "id": "fn-xBYU1URqK"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once installed, let's do our encode and decode like before but this time with tiktoken."
      ],
      "metadata": {
        "id": "f_k8nO2-jLzZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = (\n",
        "    \"Hello, do you like tea? <|endoftext|> In the sunlit terraces\"\n",
        "     \"of someunknownPlace.\"\n",
        ")\n",
        "\n",
        "integers = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
        "\n",
        "print(integers)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zFeywC8yUVYz",
        "outputId": "68ac0a49-5b9c-4563-a136-b14643277957"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 4252, 18250, 8812, 2114, 1659, 617, 34680, 27271, 13]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "strings = tokenizer.decode(integers)\n",
        "\n",
        "print(strings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U2ogjtz0UWSS",
        "outputId": "15b54a76-34df-4580-9c75-f54b78e0df2e"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello, do you like tea? <|endoftext|> In the sunlit terracesof someunknownPlace.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can make two noteworthy observations based on the token IDs and decoded text. First, the <|endoftext|> token is assigned a relatively large token ID, namely, 50256. In fact, the BPE tokenizer, which was used to train models such as GPT-2, GPT-3, and the original model used in ChatGPT, has a total vocabulary size of 50257, with <|endoftext|> being assigned the largest token ID.\n",
        "\n",
        "Second, the BPE tokenizer encodes and decodes unknown words, such as someunknownPlace, correctly. The BPE tokenizer can handle any unknown word without using <|unk|> token. BPE algorithm breakdown words that aren't predefined in the vocabulary into smaller sub word unit or even down to characters, enabling it to handle out-of-vocabulary words.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1e4ejtAvUhcvMAgzn0tlFXjcVGZ0wvMWs\">\n",
        "\n",
        "The ability to break down unknown words into individual characters ensures that the tokenizer and, consequently, the LLM that is trained with it can process any text, even if it contains words that were not present in its training data.\n",
        "\n",
        "BPE builds its vocabulary by iteratively merging frequent characters into sub words and frequent sub words into words. For example, BPE starts with adding all individual single characters to its vocabulary (\"a\", \"b\", etc.). Next, it merges character combinations that frequently occur together into sub words. For example, \"d\" and \"e\" may be merged into the subword \"de,\" which is a common word in English like \"define\", \"decode\", \"depend\", \"made\", and \"hidden\"."
      ],
      "metadata": {
        "id": "3PokaEjSjOTu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Exercise**"
      ],
      "metadata": {
        "id": "aCXlPL5sjjMu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Akwirw ier\"\n",
        "\n",
        "integers = tokenizer.encode(text)\n",
        "\n",
        "print(integers)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OkMxiCE9UX97",
        "outputId": "639fc3ed-fe2f-4c0b-d65b-ad72e6fd6dbc"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[33901, 86, 343, 86, 220, 959]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "strings = tokenizer.decode(integers)\n",
        "print(strings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UHoIHKFhbJ23",
        "outputId": "a98355e7-57fd-479c-a933-76a233264c11"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Akwirw ier\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2.6 Data Sampling with a Sliding Window**"
      ],
      "metadata": {
        "id": "isqX0AjFkhmr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next step in creating the embeddings for the LLM is to generate the input-target pairs required for training an LLM. This is what input-target pairs look like:\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1_AxTlzcmcs7jifcLw9H0t0njgOYV4nHR\">"
      ],
      "metadata": {
        "id": "5wzV1K0_xbls"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's implement a data loader that fetches the input-target pairs in above figure from the training dataset using a sliding window approach. To get started, we will tokenize the whole \"The Verdict\" short story using the BPE tokenizer.\n",
        "\n"
      ],
      "metadata": {
        "id": "RinUxNKhxowI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    raw_text = f.read()\n",
        "\n",
        "enc_text = tokenizer.encode(raw_text)\n",
        "print(len(enc_text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "utyXx4hYkmfo",
        "outputId": "ef399867-8b67-4a5e-b304-3c5e40fd59fb"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5145\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5145 is the total token from the dataset. Next, we remove the first 50 tokens from the dataset for demonstration purposes, as it results in a slightly more interesting text passage in the next steps."
      ],
      "metadata": {
        "id": "lCgTvyyexrr3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "enc_sample = enc_text[50:]"
      ],
      "metadata": {
        "id": "sWmTcQGWlkeJ"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To better understand input-target, we will create 2 variable X and Y. X is a variable list to contain token value from the dataset (input) and Y is the same as X but with shifted right by 1 (target)."
      ],
      "metadata": {
        "id": "9YdhSQbUxuk4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "context_size = 4\n",
        "\n",
        "x = enc_sample[:context_size]\n",
        "y = enc_sample[1:context_size+1]\n",
        "\n",
        "print(f\"x: {x}\")\n",
        "print(f\"y:      {y}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vefpBRxTlk8a",
        "outputId": "0204c9d1-4860-4a0c-e3b8-1963e39220d1"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x: [290, 4920, 2241, 287]\n",
            "y:      [4920, 2241, 287, 257]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "From this, we can create next-word prediction task just like our figure earlier"
      ],
      "metadata": {
        "id": "kykXFl_xxx4U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(1, context_size+1):\n",
        "    context = enc_sample[:i]\n",
        "    desired = enc_sample[i]\n",
        "\n",
        "    print(context, \"---->\", desired)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MvONGTNslmzK",
        "outputId": "819cad3d-c2ee-4370-a46c-a54fe528657b"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[290] ----> 4920\n",
            "[290, 4920] ----> 2241\n",
            "[290, 4920, 2241] ----> 287\n",
            "[290, 4920, 2241, 287] ----> 257\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(1, context_size+1):\n",
        "    context = enc_sample[:i]\n",
        "    desired = enc_sample[i]\n",
        "\n",
        "    print(tokenizer.decode(context), \"---->\", tokenizer.decode([desired]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v0bhHnhMloSo",
        "outputId": "c7ae9d1b-4ca4-40b1-e267-a03c90ec1daa"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " and ---->  established\n",
            " and established ---->  himself\n",
            " and established himself ---->  in\n",
            " and established himself in ---->  a\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we created both next-word prediction in encoded format or decoded format. We've now created the input-target pairs that we can use for LLM training.\n",
        "\n",
        "There's only one more task before we can turn the tokens into embeddings: implementing an efficient data loader that iterates over the input dataset and returns the inputs and targets as PyToch tensors, which can be thought of as multidimensional arrays. We will return two tensors:\n",
        "\n",
        "*   Input tensor: containing the text that the LLM sees.\n",
        "*   Target tensor: includes the targets for the LLM to predict.\n",
        "\n",
        "The figure below illustrates how our data loader looks like. We will show the tokens in string format for illustration purpose. The code implementation will operate on token IDs directly since the encode method of the BPE tokenizer performs both tokenization and conversion into token IDs as a single step.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1mquEyLp1QTapnPumuOUGDU7fUhfuZZe7\">"
      ],
      "metadata": {
        "id": "7dxhdBDFx04f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(\"PyTorch version:\", torch.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fWC3_6pblpzs",
        "outputId": "5062bdef-3606-4726-d69a-2e1501a6a49e"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch version: 2.6.0+cu124\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "\n",
        "class GPTDatasetV1(Dataset):\n",
        "    def __init__(self, txt, tokenizer, max_length, stride):\n",
        "        self.input_ids = []\n",
        "        self.target_ids = []\n",
        "\n",
        "        # Tokenize the entire text\n",
        "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
        "        assert len(token_ids) > max_length, \"Number of tokenized inputs must at least be equal to max_length+1\"\n",
        "\n",
        "        # Use a sliding window to chunk the book into overlapping sequences of max_length\n",
        "        for i in range(0, len(token_ids) - max_length, stride):\n",
        "            input_chunk = token_ids[i:i + max_length]\n",
        "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
        "            self.input_ids.append(torch.tensor(input_chunk))\n",
        "            self.target_ids.append(torch.tensor(target_chunk))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.input_ids[idx], self.target_ids[idx]"
      ],
      "metadata": {
        "id": "llEcBWtMlrDB"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The GPTDatasetV1 class is based on the PyTorch Dataset class and defines how individual rows are fetched from the dataset, where each row consists of a number of token IDs (based on a max_length) assigned to an input_chunk tensor. The target_chunk tensor contains the corresponding targets."
      ],
      "metadata": {
        "id": "CX0IxPLYy-g9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dataloader_v1(txt, batch_size=4, max_length=256,\n",
        "                         stride=128, shuffle=True, drop_last=True,\n",
        "                         num_workers=0):\n",
        "\n",
        "    # Initialize the tokenizer\n",
        "    tokenizer = tiktoken.get_encoding(\"gpt2\") # initializes the tokenizer\n",
        "\n",
        "    # Create dataset\n",
        "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride) # creates dataset\n",
        "\n",
        "    # Create dataloader\n",
        "    dataloader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=shuffle,\n",
        "        drop_last=drop_last, # drop_last=True drops the last batch if it is shorter\n",
        "                             # than the specified batch_size to prevent loss spikes during training\n",
        "        num_workers=num_workers # The number of CPU processes to use for preprocessing\n",
        "    )\n",
        "\n",
        "    return dataloader"
      ],
      "metadata": {
        "id": "kZim0z9Kls4q"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's test the dataloader with a batch size of 1 for an LLM with a context size of 4 to develop an intuition of how the GPTDatasetV1 class from previous code and the create_dataloader_v1 function"
      ],
      "metadata": {
        "id": "bKizpBdrtsio"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    raw_text = f.read()"
      ],
      "metadata": {
        "id": "wIdlIG8Klvnn"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataloader = create_dataloader_v1(\n",
        "    raw_text, batch_size=1, max_length=4, stride=1, shuffle=False\n",
        ")\n",
        "\n",
        "data_iter = iter(dataloader) # Converts dataloader into a Python iterator to fetch the next\n",
        "                             # entry via Python's built-in next() function\n",
        "first_batch = next(data_iter)\n",
        "print(first_batch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jai9yveblwtL",
        "outputId": "1ef05aeb-3484-473e-a4ae-c7a9db67f43e"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[tensor([[  40,  367, 2885, 1464]]), tensor([[ 367, 2885, 1464, 1807]])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first_batch variable contains two tensors: the first tensor stores the input token IDs, and the second tensor stores the target token IDs. Since the max_lenght is set to 4, each of the two tensors contains four token IDs. Input size of 4 is quite small and size of at least 256 is more common to use. To understand what is stride=1, let's create another batch:"
      ],
      "metadata": {
        "id": "6cXPRll91XjH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "second_batch = next(data_iter)\n",
        "print(second_batch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qsoqb2zflxw6",
        "outputId": "ededcd59-f7eb-43cf-a273-66cbb4231019"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[tensor([[ 367, 2885, 1464, 1807]]), tensor([[2885, 1464, 1807, 3619]])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see, we shift the token ID's for both tensor by 1. This is what stride does.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1KIXtW1p_MBpkKAt2h54l7OuuGHXyuLYG\">"
      ],
      "metadata": {
        "id": "JZzAtsw12UJA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Batch sizes of 1, as we used from the data loader earlier, are useful for illustration purposes. But in real case, we may know that small batch sizes require less memory during training but lead to more noisy model updates. Just like in regular deep learning, the batch size is a tradeoff and a hyperparameter to experiment with when training LLMs. Now let's try to increase the batch size and stride. Stride will be set to 4 to utilize the dataset fully (not a single word is skipped) and this will avoid any overlap between the batches since more overlap could lead to increased overfitting."
      ],
      "metadata": {
        "id": "2XB_HCjY4uq4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataloader = create_dataloader_v1(raw_text, batch_size=8, max_length=4, stride=4, shuffle=False)\n",
        "\n",
        "data_iter = iter(dataloader)\n",
        "inputs, targets = next(data_iter)\n",
        "print(\"Inputs:\\n\", inputs)\n",
        "print(\"\\nTargets:\\n\", targets)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4RENl39lly9U",
        "outputId": "fe7e5b8f-ecc9-445a-c7db-47225f78ee25"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inputs:\n",
            " tensor([[   40,   367,  2885,  1464],\n",
            "        [ 1807,  3619,   402,   271],\n",
            "        [10899,  2138,   257,  7026],\n",
            "        [15632,   438,  2016,   257],\n",
            "        [  922,  5891,  1576,   438],\n",
            "        [  568,   340,   373,   645],\n",
            "        [ 1049,  5975,   284,   502],\n",
            "        [  284,  3285,   326,    11]])\n",
            "\n",
            "Targets:\n",
            " tensor([[  367,  2885,  1464,  1807],\n",
            "        [ 3619,   402,   271, 10899],\n",
            "        [ 2138,   257,  7026, 15632],\n",
            "        [  438,  2016,   257,   922],\n",
            "        [ 5891,  1576,   438,   568],\n",
            "        [  340,   373,   645,  1049],\n",
            "        [ 5975,   284,   502,   284],\n",
            "        [ 3285,   326,    11,   287]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Exercise**"
      ],
      "metadata": {
        "id": "cv2PktKg3cW8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataloaderExercise1 = create_dataloader_v1(\n",
        "    raw_text, batch_size=8, max_length=2, stride=2, shuffle=False\n",
        ")\n",
        "\n",
        "data_iter = iter(dataloaderExercise1)\n",
        "inputs, targets = next(data_iter)\n",
        "print(\"Inputs:\\n\", inputs)\n",
        "print(\"\\nTargets:\\n\", targets)\n",
        "\n",
        "dataloaderExercise2 = create_dataloader_v1(\n",
        "    raw_text, batch_size=8, max_length=8, stride=2, shuffle=False\n",
        ")\n",
        "\n",
        "data_iter = iter(dataloaderExercise2)\n",
        "inputs, targets = next(data_iter)\n",
        "print(\"Inputs:\\n\", inputs)\n",
        "print(\"\\nTargets:\\n\", targets)"
      ],
      "metadata": {
        "id": "ztecpwlKDeCM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b0a6e12-20ab-4343-ac36-de946b3ed6d9"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inputs:\n",
            " tensor([[   40,   367],\n",
            "        [ 2885,  1464],\n",
            "        [ 1807,  3619],\n",
            "        [  402,   271],\n",
            "        [10899,  2138],\n",
            "        [  257,  7026],\n",
            "        [15632,   438],\n",
            "        [ 2016,   257]])\n",
            "\n",
            "Targets:\n",
            " tensor([[  367,  2885],\n",
            "        [ 1464,  1807],\n",
            "        [ 3619,   402],\n",
            "        [  271, 10899],\n",
            "        [ 2138,   257],\n",
            "        [ 7026, 15632],\n",
            "        [  438,  2016],\n",
            "        [  257,   922]])\n",
            "Inputs:\n",
            " tensor([[   40,   367,  2885,  1464,  1807,  3619,   402,   271],\n",
            "        [ 2885,  1464,  1807,  3619,   402,   271, 10899,  2138],\n",
            "        [ 1807,  3619,   402,   271, 10899,  2138,   257,  7026],\n",
            "        [  402,   271, 10899,  2138,   257,  7026, 15632,   438],\n",
            "        [10899,  2138,   257,  7026, 15632,   438,  2016,   257],\n",
            "        [  257,  7026, 15632,   438,  2016,   257,   922,  5891],\n",
            "        [15632,   438,  2016,   257,   922,  5891,  1576,   438],\n",
            "        [ 2016,   257,   922,  5891,  1576,   438,   568,   340]])\n",
            "\n",
            "Targets:\n",
            " tensor([[  367,  2885,  1464,  1807,  3619,   402,   271, 10899],\n",
            "        [ 1464,  1807,  3619,   402,   271, 10899,  2138,   257],\n",
            "        [ 3619,   402,   271, 10899,  2138,   257,  7026, 15632],\n",
            "        [  271, 10899,  2138,   257,  7026, 15632,   438,  2016],\n",
            "        [ 2138,   257,  7026, 15632,   438,  2016,   257,   922],\n",
            "        [ 7026, 15632,   438,  2016,   257,   922,  5891,  1576],\n",
            "        [  438,  2016,   257,   922,  5891,  1576,   438,   568],\n",
            "        [  257,   922,  5891,  1576,   438,   568,   340,   373]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2.7 Creating Token Embeddings**"
      ],
      "metadata": {
        "id": "sAKQ394dC9I0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The last step in preparing the input text for LLM training is to convert the token IDs into embedding vectors\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1qMReW8VUmju0kNltrTG9NNUXXFMKevcT\" >"
      ],
      "metadata": {
        "id": "l89ZsU9tDBet"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The preliminary step is we must initialize these embedding weights with random values. This initialization serves as the starting point for the LLM's learning process. In chapter 5, we will optimize the embedding weights as part of the LLM training.\n",
        "\n",
        "A continuous vector representation, or embedding, is necessary since GPT-like LLMs are deep neural networks trained with the backpropagation algorithm.\n",
        "\n",
        "Let's code how the token ID to embedding vector conversion works. We first create four input tokens with ID 2, 3, 5, and 1"
      ],
      "metadata": {
        "id": "-0D74YZ1DH_S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids = torch.tensor([2, 3, 5, 1])"
      ],
      "metadata": {
        "id": "mgl4cChnFlQ6"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For simplicity, we use only 6 words from BPE tokenizer vocabulary, and we want to create embeddings of sizes 3 (GPT-3 have the embedding size of 12288 dimensions) Using the vocab_size and output_dim, we can instantiate an embedding layer in PyTorch, setting the random seed to 123 for reproducibility purpose"
      ],
      "metadata": {
        "id": "BwgnVAp_FvVC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 6\n",
        "output_dim = 3\n",
        "\n",
        "torch.manual_seed(123)\n",
        "embedding_layer = torch.nn.Embedding(vocab_size, output_dim)"
      ],
      "metadata": {
        "id": "QG0kQRsLFmAg"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This print the embedding layer's underlying weight matrix"
      ],
      "metadata": {
        "id": "DhuJ3jltGURw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(embedding_layer.weight)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SGJAZrkPFn_z",
        "outputId": "9f97da21-d530-4ca8-aa3c-4de16a6a8111"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameter containing:\n",
            "tensor([[ 0.3374, -0.1778, -0.1690],\n",
            "        [ 0.9178,  1.5810,  1.3010],\n",
            "        [ 1.2753, -0.2010, -0.1606],\n",
            "        [-0.4015,  0.9666, -1.1481],\n",
            "        [-1.1589,  0.3255, -0.6315],\n",
            "        [-2.8400, -0.7849, -1.4096]], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The weight matrix of the embedding layer contains small, random values. These values are used during LLM training for LLM optimization. From the matrix, we can see it has 6 rows and 3 columns. For each row, there are six possible token in the vocabulary, and there is one column for each of the three embedding dimensions.\n",
        "\n",
        "Now let's apply it to a token ID to obtain the embedding vector:"
      ],
      "metadata": {
        "id": "vaYgk7QUHL4n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(embedding_layer(torch.tensor([3])))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FEPz6VXhFpbu",
        "outputId": "a35ac430-1e8e-4ee6-9aa3-c3f0effc49f3"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-0.4015,  0.9666, -1.1481]], grad_fn=<EmbeddingBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we compare the embedding vector for token ID 3 to the previous embedding matrix, we see that it is identical to the fourth row. The above prints just prints the 4th element of embedding layer's weight."
      ],
      "metadata": {
        "id": "NukIZIpoIQ3U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We've seen how to convert a single token ID into a three-dimensional embedding vector. Let's now apply that to all four input IDs (the input_ids one)"
      ],
      "metadata": {
        "id": "YnFAjeUEI7Wg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(embedding_layer(input_ids))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iWZqOS5nFqwC",
        "outputId": "2311263a-3525-4950-ccce-5df47c8b1003"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 1.2753, -0.2010, -0.1606],\n",
            "        [-0.4015,  0.9666, -1.1481],\n",
            "        [-2.8400, -0.7849, -1.4096],\n",
            "        [ 0.9178,  1.5810,  1.3010]], grad_fn=<EmbeddingBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It will print all of the embedding layer's weight based on input_ids earlier:\n",
        "[2, 3, 5, 1]\n",
        "\n",
        "Each row in this output matrix is obtained via a lookup operation from the embedding weight matrix, as illustrated on the following figure.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1j1rYRuyZbPmJF9NWxG8WpPEx6oBIx-7_\">\n",
        "\n",
        "Now we have created embedding vectors from token IDs, next we'll add a small modification to these embedding vectors to encode positional information about a token within a text"
      ],
      "metadata": {
        "id": "YKrjFaLUJGlO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2.8 Encoding Word Positions**"
      ],
      "metadata": {
        "id": "HTK8BIH5MfU4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In principle, token embeddings are a suitable input for an LLM. However, a minor shortcoming of LLMs is that their self-attention mechanisms (chapter 3) doesn't have a notion position or order for the tokens with a sequence. The way the previously introduced embedding layer works is that the same token ID always gets mapped to the same vector representation, regardless of where the token ID is positioned in the input sequence.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=12yVZl4uWOPCY1wERToHm7MNGAE4OEjVh\">"
      ],
      "metadata": {
        "id": "AQHM2hIwMh7a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In principle, the deterministic, position-independent embedding of the token ID is good for reproducibility purposes. However, since the self-attention mechanisms of LLMs itself is also position-agnostic, it is helpful to inject additional position information into the LLM.\n",
        "\n",
        "To achieve this, we can use two broad categories of position-aware embeddings: relative positional embeddings and absolute positional embeddings. Absolute positional embeddings are directly associated with specific positions in a sequence. For each position in the input sequence, a unique embedding is added to the token's embedding to convey its exact location. For instance, the first token will have a specific positional embedding, the second token another distinct embedding, and so on. As illustrated below:\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1Ahl95RtFbMQw3AUvpZEJb-3Q4LVT0C2q\">"
      ],
      "metadata": {
        "id": "1e0X2exZROJ9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instead of focusing on the absolute position of a token, the emphasis of relative postional embeddings is on the relative position or distance between tokens. This means the model learns the relationships in terms of \"how far apart\" than \"at which exact position\". The advantage here is that the model can generalize better to sequences of varying lengths, even if it hasn't seen such lengths during training.\n",
        "\n",
        "Both types of positional embeddings aim to augment the capacity of LLMs to understand the order and relationships between tokens, ensuring more accurate and context-aware predictions. The choice between them often depends on the specific application and the nature of the data being processed.\n",
        "\n",
        "OpenAI's GPT models use absolute positional embeddings that are optimized during the training process rather than being fixed or predefined like the positional encodings in the original transformer model. This optimization process is part of the model training itself. Now, let's create the initial positional embeddings to create the LLM inputs.\n"
      ],
      "metadata": {
        "id": "INhnmUPwUO-P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Previously, we focused on very small embedding sizes for simplicity. Now, let's consider more realistic and useful embedding sizes and encode the input tokens into a 256-dimensional vector representation. For vocab size, we will use the size of token IDs created by the BPE tokenizer we implemented earlier."
      ],
      "metadata": {
        "id": "TPosdRNuVuC6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 50257 # From BPE tokenizer earlier\n",
        "output_dim = 256 # embedding sizes of 256-dimensional vector representation instead just 3\n",
        "\n",
        "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)"
      ],
      "metadata": {
        "id": "z5N9VvNMWJ-D"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will use the previous token_embedding_layer and sample data from the data loader, we embed each token in each batch into a 256-dimensional vector. If we have a batch size of 8 with four tokens each, the result will be an 8x4x256 tensor."
      ],
      "metadata": {
        "id": "A917dxLzWhAp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_length = 4\n",
        "dataloader = create_dataloader_v1(\n",
        "    raw_text, batch_size=8, max_length=max_length,\n",
        "    stride=max_length, shuffle=False\n",
        ")\n",
        "data_iter = iter(dataloader)\n",
        "inputs, targets = next(data_iter)"
      ],
      "metadata": {
        "id": "Wo4Pw5O2WMwd"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Token IDs:\\n\", inputs)\n",
        "print(\"\\nInputs shape:\\n\", inputs.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vuTwf8BvWOSG",
        "outputId": "127666aa-b3a6-4879-d378-e2f55286af03"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token IDs:\n",
            " tensor([[   40,   367,  2885,  1464],\n",
            "        [ 1807,  3619,   402,   271],\n",
            "        [10899,  2138,   257,  7026],\n",
            "        [15632,   438,  2016,   257],\n",
            "        [  922,  5891,  1576,   438],\n",
            "        [  568,   340,   373,   645],\n",
            "        [ 1049,  5975,   284,   502],\n",
            "        [  284,  3285,   326,    11]])\n",
            "\n",
            "Inputs shape:\n",
            " torch.Size([8, 4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see, the token ID tensor is 8x4 dimensional, meaning that the data batch consists of eight text samples with four token each.\n",
        "\n",
        "Let's now use the embedding layer to embed these token IDs into 256-dimensional vectors:"
      ],
      "metadata": {
        "id": "lCI88BfgXqCL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "token_embeddings = token_embedding_layer(inputs)\n",
        "print(token_embeddings.shape)\n",
        "\n",
        "# uncomment & execute the following line to see how the embeddings look like\n",
        "print(token_embeddings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gv_vwTPtWQcR",
        "outputId": "96073c8d-42e0-4005-afd9-57490dd1768b"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([8, 4, 256])\n",
            "tensor([[[ 0.4913,  1.1239,  1.4588,  ..., -0.3995, -1.8735, -0.1445],\n",
            "         [ 0.4481,  0.2536, -0.2655,  ...,  0.4997, -1.1991, -1.1844],\n",
            "         [-0.2507, -0.0546,  0.6687,  ...,  0.9618,  2.3737, -0.0528],\n",
            "         [ 0.9457,  0.8657,  1.6191,  ..., -0.4544, -0.7460,  0.3483]],\n",
            "\n",
            "        [[ 1.5460,  1.7368, -0.7848,  ..., -0.1004,  0.8584, -0.3421],\n",
            "         [-1.8622, -0.1914, -0.3812,  ...,  1.1220, -0.3496,  0.6091],\n",
            "         [ 1.9847, -0.6483, -0.1415,  ..., -0.3841, -0.9355,  1.4478],\n",
            "         [ 0.9647,  1.2974, -1.6207,  ...,  1.1463,  1.5797,  0.3969]],\n",
            "\n",
            "        [[-0.7713,  0.6572,  0.1663,  ..., -0.8044,  0.0542,  0.7426],\n",
            "         [ 0.8046,  0.5047,  1.2922,  ...,  1.4648,  0.4097,  0.3205],\n",
            "         [ 0.0795, -1.7636,  0.5750,  ...,  2.1823,  1.8231, -0.3635],\n",
            "         [ 0.4267, -0.0647,  0.5686,  ..., -0.5209,  1.3065,  0.8473]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-1.6156,  0.9610, -2.6437,  ..., -0.9645,  1.0888,  1.6383],\n",
            "         [-0.3985, -0.9235, -1.3163,  ..., -1.1582, -1.1314,  0.9747],\n",
            "         [ 0.6089,  0.5329,  0.1980,  ..., -0.6333, -1.1023,  1.6292],\n",
            "         [ 0.3677, -0.1701, -1.3787,  ...,  0.7048,  0.5028, -0.0573]],\n",
            "\n",
            "        [[-0.1279,  0.6154,  1.7173,  ...,  0.3789, -0.4752,  1.5258],\n",
            "         [ 0.4861, -1.7105,  0.4416,  ...,  0.1475, -1.8394,  1.8755],\n",
            "         [-0.9573,  0.7007,  1.3579,  ...,  1.9378, -1.9052, -1.1816],\n",
            "         [ 0.2002, -0.7605, -1.5170,  ..., -0.0305, -0.3656, -0.1398]],\n",
            "\n",
            "        [[-0.9573,  0.7007,  1.3579,  ...,  1.9378, -1.9052, -1.1816],\n",
            "         [-0.0632, -0.6548, -1.0296,  ..., -0.9538, -0.5026, -0.1128],\n",
            "         [ 0.6032,  0.8983,  2.0722,  ...,  1.5242,  0.2030, -0.3002],\n",
            "         [ 1.1274, -0.1082, -0.2195,  ...,  0.5059, -1.8138, -0.0700]]],\n",
            "       grad_fn=<EmbeddingBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It's now shown that each token ID is now embeded as a 256-dimensional vector.\n",
        "\n",
        "For a GPT model's absolute embedding approach, we just need to create another embedding layer that has the same embedding dimension as the token_embedding_layer"
      ],
      "metadata": {
        "id": "xKfR2vsQYKAQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "context_length = max_length\n",
        "pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)\n",
        "\n",
        "# uncomment & execute the following line to see how the embedding layer weights look like\n",
        "print(pos_embedding_layer.weight)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rEPt5FZbWSBq",
        "outputId": "61a67054-1c9a-4ec8-fb5c-e601ef015209"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameter containing:\n",
            "tensor([[-0.8194,  0.5543, -0.8290,  ...,  0.1325,  0.2115,  0.3610],\n",
            "        [ 0.4193, -0.9461, -0.3407,  ...,  0.7930,  1.7009,  0.5663],\n",
            "        [-0.2362, -1.7187, -1.0489,  ...,  1.1218,  0.2796,  0.9912],\n",
            "        [-0.9549,  0.4699,  0.2580,  ..., -1.3689,  1.6505,  1.3488]],\n",
            "       requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The input to the pos_embeddings is usually a placeholder vector\n",
        "torch.arrange(context_length), which contains a sequence of numbers 0, 1, ..., up to the maximum input length -1. The context_length is a variable that represents the supported input size of the LLM. Here, we choose it similar to the maximum length of the input text. In practice, input text can be longer than the supported context length, in which case we have to truncate the text."
      ],
      "metadata": {
        "id": "m4O4tHfdY59W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pos_embeddings = pos_embedding_layer(torch.arange(max_length))\n",
        "print(pos_embeddings.shape)\n",
        "\n",
        "# uncomment & execute the following line to see how the embeddings look like\n",
        "print(pos_embeddings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uIUNnKHFWThJ",
        "outputId": "3346678e-9202-4200-b77a-749b1e6e1a80"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 256])\n",
            "tensor([[-0.8194,  0.5543, -0.8290,  ...,  0.1325,  0.2115,  0.3610],\n",
            "        [ 0.4193, -0.9461, -0.3407,  ...,  0.7930,  1.7009,  0.5663],\n",
            "        [-0.2362, -1.7187, -1.0489,  ...,  1.1218,  0.2796,  0.9912],\n",
            "        [-0.9549,  0.4699,  0.2580,  ..., -1.3689,  1.6505,  1.3488]],\n",
            "       grad_fn=<EmbeddingBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see, the positional embedding tensor consists of four 256-dimensional vectors. We can now add these directly to the token embeddings, where PyTorch will add the 4x256-dimensional pos_embeddings tensor to each 4x256-dimensional token embedding tensor in each of the eight batches:"
      ],
      "metadata": {
        "id": "MenAtpQqZnS9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_embeddings = token_embeddings + pos_embeddings\n",
        "print(input_embeddings.shape)\n",
        "\n",
        "# uncomment & execute the following line to see how the embeddings look like\n",
        "print(input_embeddings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-g05wy0IWU41",
        "outputId": "8e5365d8-3e72-4f30-af37-341346703665"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([8, 4, 256])\n",
            "tensor([[[-0.3281,  1.6782,  0.6298,  ..., -0.2670, -1.6620,  0.2165],\n",
            "         [ 0.8674, -0.6925, -0.6063,  ...,  1.2927,  0.5018, -0.6181],\n",
            "         [-0.4869, -1.7733, -0.3802,  ...,  2.0836,  2.6533,  0.9384],\n",
            "         [-0.0091,  1.3356,  1.8771,  ..., -1.8233,  0.9045,  1.6972]],\n",
            "\n",
            "        [[ 0.7267,  2.2912, -1.6138,  ...,  0.0321,  1.0699,  0.0189],\n",
            "         [-1.4429, -1.1375, -0.7219,  ...,  1.9150,  1.3513,  1.1754],\n",
            "         [ 1.7486, -2.3669, -1.1904,  ...,  0.7377, -0.6559,  2.4390],\n",
            "         [ 0.0099,  1.7672, -1.3627,  ..., -0.2226,  3.2302,  1.7457]],\n",
            "\n",
            "        [[-1.5907,  1.2115, -0.6627,  ..., -0.6719,  0.2657,  1.1036],\n",
            "         [ 1.2239, -0.4414,  0.9515,  ...,  2.2578,  2.1106,  0.8868],\n",
            "         [-0.1567, -3.4823, -0.4740,  ...,  3.3041,  2.1027,  0.6277],\n",
            "         [-0.5282,  0.4051,  0.8265,  ..., -1.8898,  2.9570,  2.1961]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-2.4349,  1.5153, -3.4727,  ..., -0.8320,  1.3004,  1.9994],\n",
            "         [ 0.0208, -1.8696, -1.6570,  ..., -0.3652,  0.5696,  1.5410],\n",
            "         [ 0.3728, -1.1858, -0.8510,  ...,  0.4885, -0.8227,  2.6204],\n",
            "         [-0.5872,  0.2998, -1.1208,  ..., -0.6641,  2.1533,  1.2915]],\n",
            "\n",
            "        [[-0.9473,  1.1698,  0.8883,  ...,  0.5114, -0.2637,  1.8868],\n",
            "         [ 0.9054, -2.6566,  0.1009,  ...,  0.9405, -0.1385,  2.4418],\n",
            "         [-1.1935, -1.0180,  0.3090,  ...,  3.0596, -1.6256, -0.1904],\n",
            "         [-0.7547, -0.2907, -1.2590,  ..., -1.3994,  1.2849,  1.2090]],\n",
            "\n",
            "        [[-1.7767,  1.2550,  0.5289,  ...,  2.0703, -1.6937, -0.8205],\n",
            "         [ 0.3562, -1.6009, -1.3703,  ..., -0.1608,  1.1983,  0.4535],\n",
            "         [ 0.3671, -0.8203,  1.0232,  ...,  2.6459,  0.4826,  0.6910],\n",
            "         [ 0.1726,  0.3616,  0.0384,  ..., -0.8630, -0.1632,  1.2788]]],\n",
            "       grad_fn=<AddBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The input_embeddings we created, as summarized in figure below, are the embedded input examples that can now be processed by the main LLM modules, which will be implemented next chapter.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1JDYWH7YA10fXY217b3K5Nd9fiqcXVZ5x\">"
      ],
      "metadata": {
        "id": "r6rZdW4qaGfC"
      }
    }
  ]
}