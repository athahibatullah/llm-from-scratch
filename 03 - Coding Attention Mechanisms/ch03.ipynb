{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP164aC/V6hZOnDz3uzkc1n",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/athahibatullah/llm-from-scratch/blob/main/03%20-%20Coding%20Attention%20Mechanisms/ch03.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Coding Attention Mechanisms**"
      ],
      "metadata": {
        "id": "Mm8N88xy-P5g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we will lok at an integral part of the LLM architecture itself, attention mechanisms.\n",
        "\n",
        "Figure below depicts different attention mechanisms we will code in this chapter. These different attention variants build on each other, and the goal is to arrive at a compact and efficient implementation of multi-head attention that we can then plug into the LLM architecture we will code in the next chapter.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1eY2OWPT3QfDvouOPbSP0YfV4_PzfF7EG\">"
      ],
      "metadata": {
        "id": "j3dU9277-HMo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3.1 The Problem with Modeling Long Sequences**"
      ],
      "metadata": {
        "id": "7ct9xjmu_L4k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before we dive into self-attention mechanism, we should know the problem with pre-LLM architectures that do not include attention mechanisms.\n",
        "\n",
        "Suppose we want to build a language translation model, we can't translate a text from a language to another language in the same sequence because grammar exist\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1WxOhVuKOqN-nR_RWcI8Z6E-5CscsQt0p\">\n",
        "\n",
        "To address this problem, it is common to use a deep neural network with two submodules, an encoder and a decoder. The job of the encoder is to first read in and process the entire text, and the decoder then produces the translated text.\n",
        "\n",
        "Before the advent of transformers, recurrent neural networks (RNN) were the most popular encoder-decoder architecture for language translation. An RNN is a type of neural network where outputs from previous steps are fed as inputs to the current step, making them well-suited for sequential data like text.\n",
        "\n",
        "In encoder-decoder RNN, the input text is fed into the encoder, which processes it sequentially. The encoder updates its hidden state (the internal values at the hidden layers) at each step, tryning to capture the entire meaning of the input sentence in the final hidden state. The decoder then start to take this final hidden state to start translating text one word at a time. It also updates its hidden state at each step, which is supposed to carry the context necessary for the next-word prediction.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1ouD6Ma4e4ScqF3Sr5UW7cJWsKDWnkXWj\">\n",
        "\n",
        "The key idea here is that encoder part processes the entire input text into a hidden state (memory cell). The decoder then takes in the hidden state to produce the output. Hidden state can be analogize like embedding vector.\n",
        "\n",
        "The big limitation of encoder-decoder RNNs is that the RNN can't directly access earlier hidden states from the encoder during the decoding phase. It relies solely on the current hidden state, which encapsulates all relevant information. This can lead to a loss of context, especially in complex sentences where dependencies might span long distances. It is not essential to understand RNNs to build an LLM, but the limitation of the RNNs is the motivation behind the design of attention mechanisms."
      ],
      "metadata": {
        "id": "dei4frA8_SaQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3.2 Capturing Data Dependencies with Attention Mechanisms**"
      ],
      "metadata": {
        "id": "sekzCCZBQSss"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Although RNNs work fine for translating short sentences, they don't work well for longer texts as they don't have direct access to previous words in the input. One major shortcoming in this approach is that the RNN must remember the entire encoded input in a single hidden state before passing it to the decoder.\n",
        "\n",
        "Hence, researchers developed the Bahdanau attention mechanism for RNNs in 2014, which modifies the encoder-decoder RNN such that the encoder can selectively access different parts of the input sequence at each decoding step:\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1XB4eRzWnkYccOMgoiuUIcTeXfHu49gWI\">\n",
        "\n",
        "From the image, the text-generating decoder part of the network can access all input tokens selectively. This means that some input tokens are more important than others for generating a given output token. The importance is determined by the attention weights"
      ],
      "metadata": {
        "id": "AwBqKZe6QXf3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3 years later, researchers found that RNN architectures are not required for building deep neural networks for natural language processing and proposed the original transformer architecture including a self-attention mechanism inspired by the Bahdanau attention mechanism.\n",
        "\n",
        "Self-attention is a mechanism that allows each position in the input sequence to consider the relevancy of, or \"attend to,\" all other positions in the same sequence when computing the representation of a sequence. Self-attention is a key component to LLMs based on the transformer architecture.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1eOc8sBHNafdxp-rzQpGttDBvqXDaB2kl\">"
      ],
      "metadata": {
        "id": "JZ22QG_YXmQm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3.3 Attending to Different Parts of the Input with Self-Attention**"
      ],
      "metadata": {
        "id": "DryhY5aoarAA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The \"self\" in self-attention refers to the mechanism's ability to compute attention weights by relating different positions within a single input sequence. It assess and learns the relationships and dependencies between various parts of the input itself, such as words in a sentence or pixels in an image.\n",
        "\n",
        "This is contrast to traditional attention mechanisms, where the focus is on the relationships between elements of two different sequences, such as in sequence-to-sequence models where the attention might be between an input sequence and an output sequence."
      ],
      "metadata": {
        "id": "eRMH2a8kaxcu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3.3.1 A Simple Self-Attention Mechanism without Trainable Weights**"
      ],
      "metadata": {
        "id": "i5Aj1wTjc1K_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's begin by implementing a simplified variant of self-attention, free from any trainable weights, as summarized in figure below. The goal is to illustrate a few key concepts in self-attention before adding trainable weights.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1EzLfE00OGTZmrB2N1-aBu-OKVCMTcTkJ\">\n",
        "\n",
        "Figure above shows an input sequence, denoted as x, consisting of T elements represented as x1 to xT. This sequence typically represents text, such as sentence, that has already been transformed into token embeddings."
      ],
      "metadata": {
        "id": "Pw7LI4oTc8No"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In self-attention, our goal is to calculate context vectors zi for each element xi in the input sequence. A context vector can be interpreted as an enriched embedding vector.\n",
        "\n",
        "To illustrate this concept by using above figure, let's focus on the embedding vector of the second input element, x2 (\"journey\"), and the corresponding context vector, z2. This enhanced context vector, z2, is an embedding that contains information about x2 and all other input elements, x1 to xT.\n",
        "\n",
        "Context vectors play a crucial role in self-attention. Their purpose is to create enriched representations of each element in an input sequence (like a sentence) by incorporating information from all other elements in the sequence. This is essential for LLM since LLM require relationship and relevance of words to understand context. We will add trainable weights that help an LLM learn to construct these context vectors. For now, let's implement simplified self-attention mechanisms to compute these weights and the resulting context vector one step at a time."
      ],
      "metadata": {
        "id": "MS6gMuYOikf9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Consider the following input sentence, which has already been embedded into three-dimensional vectors."
      ],
      "metadata": {
        "id": "TzN01jqE0NU6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "inputs = torch.tensor(\n",
        "  [[0.43, 0.15, 0.89], # Your     (x^1)\n",
        "   [0.55, 0.87, 0.66], # journey  (x^2)\n",
        "   [0.57, 0.85, 0.64], # starts   (x^3)\n",
        "   [0.22, 0.58, 0.33], # with     (x^4)\n",
        "   [0.77, 0.25, 0.10], # one      (x^5)\n",
        "   [0.05, 0.80, 0.55]] # step     (x^6)\n",
        ")"
      ],
      "metadata": {
        "id": "tw8AzUtXzlqK"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first step of implementing self-attention is to compute the intermediate values ω, referred to as attention scores (figure below). Due to spatial constraints, the figure displayed the values in truncated version. For example 0.87 is truncated to 0.8.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1ZNs_Nw-pfGhBtfqbrsUlSfitAWtZj4-3\">"
      ],
      "metadata": {
        "id": "Yn4hSUng0etE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From figure above (with truncated values), we can determine attention scores between the query token and each input token by computing the dot product of the query, x2, with every other input token:"
      ],
      "metadata": {
        "id": "LlrjpfbI3uiZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = inputs[1]  # 2nd input token is the query\n",
        "\n",
        "attn_scores_2 = torch.empty(inputs.shape[0])\n",
        "for i, x_i in enumerate(inputs):\n",
        "    attn_scores_2[i] = torch.dot(x_i, query) # dot product (transpose not necessary here since they are 1-dim vectors)\n",
        "\n",
        "print(attn_scores_2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sX-mLYZzz1NB",
        "outputId": "58fec729-96b3-47ad-9435-11029146ab4b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "How to calculate dot product:\n",
        "\n",
        "dot(a,b) = a1xb1 + a2xb2 + a3xb3 + .... + anxbn\n",
        "\n",
        "for example (using our earlier input vector):\n",
        "\n",
        "dot(input_x1,query_x2) = dot([0.43, 0.15, 0.89],[0.55, 0.87, 0.66])\n",
        "\n",
        "                        = (0.43x0.55)+(0.15x0.87)+(0.89x0.66)\n",
        "                        = 0.9544\n"
      ],
      "metadata": {
        "id": "Kgtcl4EH5VyW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "res = 0.\n",
        "\n",
        "for idx, element in enumerate(inputs[0]):\n",
        "    res += inputs[0][idx] * query[idx]\n",
        "\n",
        "print(res)\n",
        "print(torch.dot(inputs[0], query))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yltPZHYaz1g1",
        "outputId": "a5d4a631-d3c8-4801-954d-cc3edb66fe03"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.9544)\n",
            "tensor(0.9544)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dot product is a measure of similarity because it quantifies how closely two vectors are aligned: a higher dot product indicates a greater degree of alignment or similarity between the vectors. In the context of self-attention mechanisms, the dot product determines the extent to which each element in a sequence focuses on, or \"attends to,\" any other element: the higher the dot product, the higher the similarity and attention score between two elements."
      ],
      "metadata": {
        "id": "BNg6TvFV6ZLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the next step, we normalize each of the attention scores we computed previously. The main goal behind the normalization is to obtain the attention weights that sum up to 1. This normalization is a convention that is useful for interpretation and maintaning training stability in an LLM."
      ],
      "metadata": {
        "id": "3-rRDXVs7WHX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "attn_weights_2_tmp = attn_scores_2 / attn_scores_2.sum()\n",
        "\n",
        "print(\"Attention weights:\", attn_weights_2_tmp)\n",
        "print(\"Sum:\", attn_weights_2_tmp.sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gl_TJNlsz2i-",
        "outputId": "47a6f2ae-edf6-4428-91f9-fc17f8bcfe3f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention weights: tensor([0.1455, 0.2278, 0.2249, 0.1285, 0.1077, 0.1656])\n",
            "Sum: tensor(1.0000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the output, the attention weights now sum to 1\n",
        "<img src=\"https://drive.google.com/uc?id=1jvu08NA_G2vb1r9SHeyKMP59UkZoiNei\">"
      ],
      "metadata": {
        "id": "PFJpB3Q7_72L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In practice, it's more common and advisable to use the softmax function for normalization. This approach is better at managing extreme values and offers more favorable gradient properties during training. The following is a basic implementation of the softmax function for normalizing the attention scores:"
      ],
      "metadata": {
        "id": "TR5FjmnwBzuv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax_naive(x):\n",
        "    return torch.exp(x) / torch.exp(x).sum(dim=0)\n",
        "\n",
        "attn_weights_2_naive = softmax_naive(attn_scores_2)\n",
        "\n",
        "print(\"Attention weights:\", attn_weights_2_naive)\n",
        "print(\"Sum:\", attn_weights_2_naive.sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o0TXz5zkz43S",
        "outputId": "804f954f-7da5-4792-8354-c16b18aaced4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention weights: tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
            "Sum: tensor(1.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As the output shows, the softmax function also meets the objective and normalizes the attention weights such that they sum to 1"
      ],
      "metadata": {
        "id": "KEcuu9QbCI-4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In addition, the softmax function ensures that the attention weights are always positive. This make the output interpretable as probabilities or relative importance, where higher weights indicate greater importance.\n",
        "\n",
        "Note that this naive softmax implementation (softmax_naive) may encounter numerical instability problems, such as overflow and underflow, when dealing with large or small input values. Therefore, in practice, it's advisable to use the PyTorch implementation of softmax, which has been extensively optimized for performance:"
      ],
      "metadata": {
        "id": "hETACGIbCgxd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "attn_weights_2 = torch.softmax(attn_scores_2, dim=0)\n",
        "\n",
        "print(\"Attention weights:\", attn_weights_2)\n",
        "print(\"Sum:\", attn_weights_2.sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "StTrmj8qz6Zy",
        "outputId": "94c568a3-2d2f-4562-db7c-d834ecc3ddb7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention weights: tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
            "Sum: tensor(1.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After computing the normalized attention weights, the last step is to calculate the context vector z2 by multiplying the embedded input tokens, xi, with the corresponding attention weights and then summing the resulting vectors. Thus, the context vector z2 is the weighted sum of all input vectors, obtained by multiplying each input vector by its corresponding attention weight:"
      ],
      "metadata": {
        "id": "lMUQbqnDDDJw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = inputs[1] # 2nd input token is the query\n",
        "context_vec_2 = torch.zeros(query.shape)\n",
        "for i,x_i in enumerate(inputs):\n",
        "    print(\"attention weights: {} | embedded input token: {}\".format(attn_weights_2[i], x_i))\n",
        "    context_vec_2 += attn_weights_2[i]*x_i\n",
        "\n",
        "print(context_vec_2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fAVNBm4Hz7kt",
        "outputId": "d1890239-9be3-4e0f-e532-d4a7802f2707"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "attention weights: 0.13854756951332092 | embedded input token: tensor([0.4300, 0.1500, 0.8900])\n",
            "attention weights: 0.2378913015127182 | embedded input token: tensor([0.5500, 0.8700, 0.6600])\n",
            "attention weights: 0.23327402770519257 | embedded input token: tensor([0.5700, 0.8500, 0.6400])\n",
            "attention weights: 0.12399158626794815 | embedded input token: tensor([0.2200, 0.5800, 0.3300])\n",
            "attention weights: 0.10818186402320862 | embedded input token: tensor([0.7700, 0.2500, 0.1000])\n",
            "attention weights: 0.15811361372470856 | embedded input token: tensor([0.0500, 0.8000, 0.5500])\n",
            "tensor([0.4419, 0.6515, 0.5683])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://drive.google.com/uc?id=1dwjwcsif32LI_glFql5zSkLWYKhJP0D4\">\n",
        "\n",
        "Next, we will generalize the procedure for computing context vectors to calculate all context vectors simultaneously"
      ],
      "metadata": {
        "id": "Z1jPJ7ewG2WJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Softmax Calculation**\n",
        "\n",
        "attention score: tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])\n",
        "<img src=\"https://drive.google.com/uc?id=1L4uScFhdELfzTLmVPIVWQMI8n8Mz0_OV\">\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1gZe12xpukp5bjUTbi6WU7ynpQNWgsHj8\">\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1O5A0-iaAJ0S5ErbH1DZq8X3M25rA834a\">\n",
        "\n",
        "The result of softmax:\n",
        "attention weight: tensor([0.1386, 0.2378, 0.2332, 0.1240, 0.1082, 0.1582])"
      ],
      "metadata": {
        "id": "MzqWU_9MOrXo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3.3.2 Computing Attention Weights for All Input Tokens**"
      ],
      "metadata": {
        "id": "_sUdKx4oPkY1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "So far, we have computed attention weights and the context vector for input 2, as shown in the highlighted row in below figure. Now let's extend this computation to calculate attention weights and context vectors for all inputs\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1LNedYJwe7f5b3sRMVdkINtCffWb55Msa\">\n",
        "\n",
        "We follow the same three steps as before, except that we make a few modifications in the code to compute all context vectors instead of only the second one, z2"
      ],
      "metadata": {
        "id": "jD4Pzcu-P4u0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "attn_scores = torch.empty(6, 6)\n",
        "\n",
        "for i, x_i in enumerate(inputs):\n",
        "    for j, x_j in enumerate(inputs):\n",
        "        attn_scores[i, j] = torch.dot(x_i, x_j)\n",
        "\n",
        "print(attn_scores)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L1QSXnE6UkTV",
        "outputId": "34063a44-49bf-4bd7-be1a-773d9b4f3c87"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
            "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
            "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
            "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
            "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
            "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://drive.google.com/uc?id=1QEHg5PWEmc_bBkojiWHGDY2xKvEJLvho\">"
      ],
      "metadata": {
        "id": "3Slj_ZB4VuSt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each element in the tensor represents an attention score between each pair of inputs.\n",
        "\n",
        "When computing the preceding attention score tensor, we used for loops in Python. However, for loops are generally slow, and we can achieve the same results using matrix multiplication:"
      ],
      "metadata": {
        "id": "TPjBBC0BV4h1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "attn_scores = inputs @ inputs.T\n",
        "print(attn_scores)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lx1LAEV8UobE",
        "outputId": "455e80c5-7b29-4a5f-aa8a-37359e9236d5"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
            "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
            "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
            "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
            "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
            "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, let's normalize these attention scores using softmax function so the value of each row is sum to 1:"
      ],
      "metadata": {
        "id": "s8LxXXnbWnGy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "attn_weights = torch.softmax(attn_scores, dim=-1)\n",
        "print(attn_weights)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_5-ZLhcmUqFM",
        "outputId": "09a4884d-9a0f-4b9f-fd12-d4f1b36e6267"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.2098, 0.2006, 0.1981, 0.1242, 0.1220, 0.1452],\n",
            "        [0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581],\n",
            "        [0.1390, 0.2369, 0.2326, 0.1242, 0.1108, 0.1565],\n",
            "        [0.1435, 0.2074, 0.2046, 0.1462, 0.1263, 0.1720],\n",
            "        [0.1526, 0.1958, 0.1975, 0.1367, 0.1879, 0.1295],\n",
            "        [0.1385, 0.2184, 0.2128, 0.1420, 0.0988, 0.1896]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the context of using PyTorch, the dim parameter in functions like torch.softmax specifies the dimension of the input tensor along which the function will be computed. By setting dim=-1, we are instructing the softmax function to apply the normalization along the last dimension of the attn_scores tensor. If attn_scores is a two-dimensional tensor (for example, with a shape of [rows, columns]), it will normalize accross the columns so that the values in each row (summing over the column dimension) sum up to 1.\n",
        "\n",
        "We can verify that the rows indeed all sum to 1:"
      ],
      "metadata": {
        "id": "2Ce4Sy8lW6Iu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "row_2_sum = sum([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
        "print(\"Row 2 sum:\", row_2_sum)\n",
        "\n",
        "print(\"All row sums:\", attn_weights.sum(dim=-1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wS0mUJ86Ur2d",
        "outputId": "5e3260dd-bcb4-4997-a8d1-810a92524c29"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Row 2 sum: 1.0\n",
            "All row sums: tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Last step, we use these attention weights to calculate the context vector via matrix multiplication:"
      ],
      "metadata": {
        "id": "vm2BTFcIXqdt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_context_vecs = attn_weights @ inputs\n",
        "print(all_context_vecs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ExJlWLzgUuE7",
        "outputId": "d1266d33-4cef-4cae-9ddf-274178fd842a"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.4421, 0.5931, 0.5790],\n",
            "        [0.4419, 0.6515, 0.5683],\n",
            "        [0.4431, 0.6496, 0.5671],\n",
            "        [0.4304, 0.6298, 0.5510],\n",
            "        [0.4671, 0.5910, 0.5266],\n",
            "        [0.4177, 0.6503, 0.5645]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can double-check that the code is correct by comparing the second row with the context vector z2 that we computed in section 3.3.1:"
      ],
      "metadata": {
        "id": "NcWZ72drYDwp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Previous 2nd context vector:\", context_vec_2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0hEiLjp5UvoN",
        "outputId": "99666f0c-2d22-419b-e1ed-a676a4a8bd65"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Previous 2nd context vector: tensor([0.4419, 0.6515, 0.5683])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on the result, we can see the exact same result for context vector z2.\n",
        "\n",
        "This concludes the code walkthrough of a simple self-attention mechanism. Next, we will add trainable weights, enabling the LLM to learn from data and improve its performance on specific tasks."
      ],
      "metadata": {
        "id": "xSqVgfYiYNng"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3.4 Implementing Self-Attention with Trainable Weights**"
      ],
      "metadata": {
        "id": "JhPQK5O8YfBa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our next step will be to implement the self-attention mechanism used in the original transformer architecture, the GPT models, and most other popular LLMs. This self-attention mechanism is also called scaled dot-product attention. Figure below shows how this self-attention mechanism fits into broader context of implementing an LLM.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1Ii-dv8sAL3SEQXFvCG95CZiCXjMBCyhY\">\n"
      ],
      "metadata": {
        "id": "uUDSYyjQ9lHL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The self-attention mechanism with trainable weights build on the previous concepts: we want to compute context vectors as weighted sums over the input vectors specific to a certain input element. There are only slight difference compared to our simplified self-attention earlier.\n",
        "\n",
        "The most notable difference is the introduction of weight matrices that are updated during model training. These trainable weight matrices are crucial so that the model (specifically, the attention module inside the model) can learn to produce \"good\" context vectors.\n",
        "\n",
        "We will divide the self-attention mechanism into 2 parts.\n",
        "* First, we will code it step by step as before.\n",
        "* Second, we will organize the code into a compact Python class that can be imported into the LLM architecture."
      ],
      "metadata": {
        "id": "br9FBlk9_ZSK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3.4.1 Computing the Attention Weights Step by Step**"
      ],
      "metadata": {
        "id": "hSS3FIzQAeHq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will implement the self-attention mechanism step by step by introducing the three trainable weight matrices Wq, Wk, and Wv. There matrices are used to project the embedded input tokens, xi, into query, key, and value vectors, respectively, as illustrated in figure below\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1KVfx4qwvYCe9mAC6mj3Xp_yK_STyhGwE\">"
      ],
      "metadata": {
        "id": "sqqSqySVAjCR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Earlier, we defined the second input element x2 as the query when we computed the simplified attention weights to compute the context vector z2. Then we generalized this to compute all context vectors z1 ... zT for the six-word input sentence. \"Your journey starts with one step.\"\n",
        "\n",
        "Similarly, we start here by computing only one context vector, z2, for illustration purposes. We will then modify this code to calculate all context vectors.\n",
        "\n",
        "Let's begin by defining a few variables:"
      ],
      "metadata": {
        "id": "TuaUZnysDiij"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_2 = inputs[1] # second input element\n",
        "d_in = inputs.shape[1] # the input embedding size, d=3\n",
        "d_out = 2 # the output embedding size, d=2"
      ],
      "metadata": {
        "id": "S7wm6JpcHsFY"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the GPT-like models, the input and output dimensions are usually the same, but to better follow the computation, we'll use different input (d_in=3) and output (d_out=2) dimensions here.\n",
        "\n",
        "Next, we intialize the three weight matrices Wq, Wk, and Wv."
      ],
      "metadata": {
        "id": "0hhtjlaBIB_n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "\n",
        "W_query = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
        "W_key   = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
        "W_value = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
        "\n",
        "print(\"W_query: {}\".format(W_query))\n",
        "print(\"W_key: {}\".format(W_key))\n",
        "print(\"W_value: {}\".format(W_value))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IH1xmvG_HxZO",
        "outputId": "bd3a13ed-582c-4f61-a078-bba69c08d9cf"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W_query: Parameter containing:\n",
            "tensor([[0.2961, 0.5166],\n",
            "        [0.2517, 0.6886],\n",
            "        [0.0740, 0.8665]])\n",
            "W_key: Parameter containing:\n",
            "tensor([[0.1366, 0.1025],\n",
            "        [0.1841, 0.7264],\n",
            "        [0.3153, 0.6871]])\n",
            "W_value: Parameter containing:\n",
            "tensor([[0.0756, 0.1966],\n",
            "        [0.3164, 0.4017],\n",
            "        [0.1186, 0.8274]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We set requires_grad=False to reduce clutter in the outputs, but if we were to use the weight matrices for model training, we would set requires_grad=True to update these matrices during model training.\n",
        "\n",
        "Next, we compute the query, key, and value vectors:"
      ],
      "metadata": {
        "id": "WAoUsNLqI3_-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query_2 = x_2 @ W_query # _2 because it's with respect to the 2nd input element\n",
        "key_2 = x_2 @ W_key\n",
        "value_2 = x_2 @ W_value\n",
        "\n",
        "print(query_2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bnhOFzHIHyFH",
        "outputId": "3609fc05-1d10-4125-bbbd-f8a5d1316e9a"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.4306, 1.4551])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The output for the query results in a two-dimensional vector since we set the number of columns of the corresponding weight matrix, via d_out, to 2."
      ],
      "metadata": {
        "id": "rdOzaoN1KBzt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Weight parameters vs. attention weights**\n",
        "\n",
        "In the weight matrices W, the term \"weight\" is short for \"weight parameters,\" the values of a neural network that are optimized during training. This is not to be confused with the attention weights. Attention weights determine the extent to which a context vector depends on the different parts of the input.\n",
        "\n",
        "In summary, weight parameters are the fundamental, learned coefficients that define the network's connections, while attention weights are dynamic, context-specific values."
      ],
      "metadata": {
        "id": "-jtmcnKaKPsc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Even though our temporary goal is only compute the one context vector, z2, we still require the key and value vectors for all input elements as they are involved in computing the attention weights with respect to the query q2.\n",
        "\n",
        "We can obtain all keys and values via matrix multiplication:"
      ],
      "metadata": {
        "id": "6S8LSmoWLAtH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "keys = inputs @ W_key\n",
        "values = inputs @ W_value\n",
        "\n",
        "print(\"keys.shape:\", keys.shape)\n",
        "print(\"values.shape:\", values.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1BGVgxvrHziP",
        "outputId": "7978f8c5-0fff-4581-9cb7-eebe71391fde"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "keys.shape: torch.Size([6, 2])\n",
            "values.shape: torch.Size([6, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We successfully projected the six input tokens from a three-dimensional onto a two-dimensional embedding space"
      ],
      "metadata": {
        "id": "y37X_vysLb8-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The second step is to compute the attention scores:\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1pQN0FaxIuyl_KCeR54jx9c-CDi0qksyd\">\n",
        "The attention score computation is a dot-product computation similar to what we used in the simplified self-attention mechanism in section 3.3. The new aspect here is that we are not directly computing the dot-product between the input elements but using the query and key obtained by transforming the inputs via the respective weight matrices.\n",
        "\n",
        "First, let's compute the attention score ω22:"
      ],
      "metadata": {
        "id": "ge0HrIZ1NmRQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "keys_2 = keys[1] # Python starts index at 0\n",
        "attn_score_22 = query_2.dot(keys_2)\n",
        "print(attn_score_22)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wnwew0tXH0du",
        "outputId": "97be45f1-b144-4e6d-e8e8-25ced72a28a0"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(1.8524)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Again, we can generalize this computation to all attention scores via matrix multiplication:"
      ],
      "metadata": {
        "id": "dYhhF9mjRKnE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "attn_scores_2 = query_2 @ keys.T # All attention scores for given query\n",
        "print(attn_scores_2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OM5FVGNvH1vB",
        "outputId": "53b2a089-ad2e-4d95-ef17-6aa454391e78"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1.2705, 1.8524, 1.8111, 1.0795, 0.5577, 1.5440])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we get the attention scores, we need to calculate the attention weight. We compute the attention weights by scaling the attention scores and using the softmax function. However, now we scale the attention scores by dividing them by the square root of the embedding dimension of the keys (taking the square root is mathematically the same as exponentiating by 0.5)\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1XeZfgHyyNZbAu9-oWRmN727g8q0Hy5gE\">"
      ],
      "metadata": {
        "id": "kRqCctDBRu0y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "d_k = keys.shape[1]\n",
        "print(d_k)\n",
        "attn_weights_2 = torch.softmax(attn_scores_2 / d_k**0.5, dim=-1)\n",
        "print(attn_weights_2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z4YNisbqH244",
        "outputId": "0bd3bede-3f9a-4e69-ceb3-798a663dcc0b"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n",
            "tensor([0.1500, 0.2264, 0.2199, 0.1311, 0.0906, 0.1820])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The rationale behind scaled-dot product attention**\n",
        "\n",
        "The reason for the normalization by the embedding dimension size is to improve the training performance by avoiding small gradients. For instance, when scaling up the embedding dimension, which is typically greater than 1000 for GPT-like LLMs, large dot products can result in very small gradients during backpropagation due to the softmax function applied to them. As dot products increase, the softmax function behaves more like a step function, resulting in gradients nearing zero. These small gradients can drastically slow down learning or cause training to stagnate.\n",
        "\n",
        "The scaling by the square root of the embedding dimension is the reason why this self-attention mechanism is also called scaled-dot product attention"
      ],
      "metadata": {
        "id": "BpyucxPuUNZm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, the final step is to compute the context vectors:\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1FlP3au_su_2eX_to_uAyPCFlSi0Y-18_\">"
      ],
      "metadata": {
        "id": "ZR_e01nJU6fZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Similar to when we computed the context vector as a weighted sum over the input vectors, we now compute the context vector as a weighted sum over the value vectors. Here, the attention weights serve as a weighting factor that weighs the respective importance of each value vector. Also as before, we can use matrix multiplication:"
      ],
      "metadata": {
        "id": "OfBe8flxXi97"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "context_vec_2 = attn_weights_2 @ values\n",
        "print(context_vec_2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pF9X_Pm6H4A6",
        "outputId": "a338bf71-3067-4ffe-8bdf-193ecab5a48b"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.3061, 0.8210])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "So far, we've only computed a single context vector, z2. Next, we will generalize the code to compute all context vectors in the input sequence, z1 to zT."
      ],
      "metadata": {
        "id": "iJ6QTJY0X7Za"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Why query, key, and value?**\n",
        "\n",
        "The terms \"key\", \"query\", and \"value\" in the context of the attention mechanisms are borrowed from the domain of information retrieval and databases, where similar concepts are used to store, search, and retrieve information.\n",
        "\n",
        "A query is analogous to a search query in a database. It represents the current item (e.g. a word or token in a sentence) the model focuses on or tries to understand. The query is used to probe the other parts of the input sequence to determine how much attention to pay to them.\n",
        "\n",
        "The key is like a database key used for indexing and searching. In the attention mechanism, each item in the input sequence (e.g., each word in a sentence) has an associated key. These keys are used to match the query.\n",
        "\n",
        "The value in this context is similar to the value in a key-value pair in a database. It represents the actual content or representation of the input items. Once the model determines which keys (and thus which parts of the input) are most relevant to the query (the current focus item), it retrieves the corresponding values.\n"
      ],
      "metadata": {
        "id": "H700Kg7hYJDo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3.4.2 Implementing a Compact Self-Attention Python Class**"
      ],
      "metadata": {
        "id": "-BSwz8KwZbVJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we need to implement all we learned before into a Python class"
      ],
      "metadata": {
        "id": "d_o0WudyZ0YT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class SelfAttention_v1(nn.Module):\n",
        "\n",
        "    def __init__(self, d_in, d_out):\n",
        "        super().__init__()\n",
        "        self.W_query = nn.Parameter(torch.rand(d_in, d_out))\n",
        "        self.W_key   = nn.Parameter(torch.rand(d_in, d_out))\n",
        "        self.W_value = nn.Parameter(torch.rand(d_in, d_out))\n",
        "\n",
        "    def forward(self, x):\n",
        "        keys = x @ self.W_key\n",
        "        queries = x @ self.W_query\n",
        "        values = x @ self.W_value\n",
        "\n",
        "        attn_scores = queries @ keys.T # omega\n",
        "        attn_weights = torch.softmax(\n",
        "            attn_scores / keys.shape[-1]**0.5, dim=-1\n",
        "        )\n",
        "\n",
        "        context_vec = attn_weights @ values\n",
        "        return context_vec"
      ],
      "metadata": {
        "id": "t9iVXyb6Zn6c"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this PyTorch code, SelfAttention_v1 is a class derived from nn.Module, which is a fundamental building block of PyTorch models that provides necessary functionalities for model layer creation and management.\n",
        "\n",
        "The __init__ method initializes trainable weights matrices (W_query, W_key, and W_value) for queries, keys, and values, each transforming the input dimension d_in to an output dimension d_out.\n",
        "\n",
        "During the forward pass, using the forward method, we compute attention scores (attn_scores) by multiplying queries and keys, normalizing these scores using softmax. Finally, we create a context vector by weighting the values with these normalized attention scores.\n",
        "\n",
        "This is how to implement the class:"
      ],
      "metadata": {
        "id": "EQNNREaGaES6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "sa_v1 = SelfAttention_v1(d_in, d_out)\n",
        "print(sa_v1(inputs))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BVtDEDOmbBNZ",
        "outputId": "61811aa8-2627-4db9-96e2-b82497c1efe2"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.2996, 0.8053],\n",
            "        [0.3061, 0.8210],\n",
            "        [0.3058, 0.8203],\n",
            "        [0.2948, 0.7939],\n",
            "        [0.2927, 0.7891],\n",
            "        [0.2990, 0.8040]], grad_fn=<MmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since inputs contains six embedding vectors, this results in a matrix storing the six context vectors.\n",
        "\n",
        "We can improve the SelfAttention_v1 implementation further by utilizing PyTorch's nn.Linear layers, which effectively perform matrix multiplication when the bias units are disabled. Additional advantage is instead of manually implementing nn.Parameter(torch.rand(...)) is that nn.Linear has an optimized weight initialization scheme, contributing to more stable and effective model training\n",
        "\n",
        "<figure>\n",
        "<img src=\"https://drive.google.com/uc?id=1M2BCfz7LSWmp_TtG4VGU9MZrTDN-pYkF\">\n",
        "<figcaption>Figure 3.4.2.1</figcaption>\n",
        "</figure>\n"
      ],
      "metadata": {
        "id": "8-TH9E48bJ-d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SelfAttention_v2(nn.Module):\n",
        "\n",
        "    def __init__(self, d_in, d_out, qkv_bias=False):\n",
        "        super().__init__()\n",
        "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        keys = self.W_key(x)\n",
        "        queries = self.W_query(x)\n",
        "        values = self.W_value(x)\n",
        "\n",
        "        attn_scores = queries @ keys.T\n",
        "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
        "\n",
        "        context_vec = attn_weights @ values\n",
        "        return context_vec\n",
        "\n",
        "torch.manual_seed(789)\n",
        "sa_v2 = SelfAttention_v2(d_in, d_out)\n",
        "print(sa_v2(inputs))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4hP6pTlRZruF",
        "outputId": "ecae3bfd-7227-4a82-ffbb-e2198181f530"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-0.0739,  0.0713],\n",
            "        [-0.0748,  0.0703],\n",
            "        [-0.0749,  0.0702],\n",
            "        [-0.0760,  0.0685],\n",
            "        [-0.0763,  0.0679],\n",
            "        [-0.0754,  0.0693]], grad_fn=<MmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We only change the init part where we intialize the weight of query, key, and value.\n",
        "\n"
      ],
      "metadata": {
        "id": "og8qv4sGg21a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we will make enhancements to the self-attention mechanism, focusing specifically on incorporating causal and multi-head elements. The causal aspect involves modifying the attention mechanism to prevent the model from accessing the future information in the sequence, which is crucial for tasks like language modeling, where each word prediction should only depend on previous words.\n",
        "\n",
        "The multi-head component involves splitting the attention mechanism into multiple \"heads.\" Each head learns different aspects of the data, allowing the model to simultaneously attend to information from different representation subspaces at different positions. This improves the model's performance in complex tasks."
      ],
      "metadata": {
        "id": "R7HA1ZdQhQ1L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3.5 Hiding Future Words with Causal Attention**"
      ],
      "metadata": {
        "id": "n-AWSDzuFpcA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For many LLM tasks, you will want the self-attention mechanism to consider only the tokens that appear prior to the current position when predicting the next token in a sequence. Causal attention, also known as masked attention, is a specialized form of self-attention. It restricts a model to only consider previous and current inputs in a sequence when processing any given token when computing attention scores. This is in contrast to the standard self-attention mechanism, which allows access to the entire input sequence at once.\n",
        "\n",
        "Now, we will modify the standard self-attention mechanism to apply the causal attention mechanism. To achive this in GPT-like LLMs, for each token processed, we mask out the future toknes, which come after the current token in the input text.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1WSykpPcit9ZSoqv1X66f_CWUqUEJslk3\">\n",
        "\n",
        "We mask out the attention weights above the diagonal, and we normalize the nonmasked attention weights such that the attention weights sum to 1 in each row. Later, we will implement this masking and normalization procedure in code."
      ],
      "metadata": {
        "id": "Bpu8ExCkF8wU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3.5.1 Applying a Causal Attention Mask**"
      ],
      "metadata": {
        "id": "J8rSDqSjKFPG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To implement the steps to apply a causal attention mask to obtain the masked attention weights, as summarized in below figure, let's work with the attention scores and weights from the previous section to code the causal attention mechanism.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1f-3QQmb0tQrBcZYf6oAwEiqyiRrZKcSa\">\n",
        "\n",
        "In the first step, we compute the attention weights using the softmax function as we have done previously:"
      ],
      "metadata": {
        "id": "ySLDr2fEKH9Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reuse the query and key weight matrices of the\n",
        "# SelfAttention_v2 object from the previous section for convenience\n",
        "queries = sa_v2.W_query(inputs)\n",
        "keys = sa_v2.W_key(inputs)\n",
        "attn_scores = queries @ keys.T\n",
        "\n",
        "attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
        "print(attn_weights)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5bazLma4MKa6",
        "outputId": "fb04d480-f220-40d5-f449-d0ab035da365"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.1921, 0.1646, 0.1652, 0.1550, 0.1721, 0.1510],\n",
            "        [0.2041, 0.1659, 0.1662, 0.1496, 0.1665, 0.1477],\n",
            "        [0.2036, 0.1659, 0.1662, 0.1498, 0.1664, 0.1480],\n",
            "        [0.1869, 0.1667, 0.1668, 0.1571, 0.1661, 0.1564],\n",
            "        [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.1585],\n",
            "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
            "       grad_fn=<SoftmaxBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can implement the second step using PyTorch's stril function to create a mask where the values above the diagonal are zero:"
      ],
      "metadata": {
        "id": "s4KGeG11Nz2v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "context_length = attn_scores.shape[0]\n",
        "mask_simple = torch.tril(torch.ones(context_length, context_length))\n",
        "print(mask_simple)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9sCqVV_MNEGm",
        "outputId": "480c48e4-bde1-4111-9a2a-89a850e98623"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1., 0., 0., 0., 0., 0.],\n",
            "        [1., 1., 0., 0., 0., 0.],\n",
            "        [1., 1., 1., 0., 0., 0.],\n",
            "        [1., 1., 1., 1., 0., 0.],\n",
            "        [1., 1., 1., 1., 1., 0.],\n",
            "        [1., 1., 1., 1., 1., 1.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we can multiply this mask with the attention weights to zero-out the values above the diagonal:"
      ],
      "metadata": {
        "id": "Mxbb1DtWOMKk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "masked_simple = attn_weights*mask_simple\n",
        "print(masked_simple)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XDHRcTpjNHgM",
        "outputId": "e5bfe86f-b397-4f92-c559-b2752522c20a"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.1921, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.2041, 0.1659, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.2036, 0.1659, 0.1662, 0.0000, 0.0000, 0.0000],\n",
            "        [0.1869, 0.1667, 0.1668, 0.1571, 0.0000, 0.0000],\n",
            "        [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.0000],\n",
            "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
            "       grad_fn=<MulBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The third step is to renormalize the attention weights to sum up to 1 again in each row. We can achieve this by dividing each row by the sum of each row"
      ],
      "metadata": {
        "id": "D1kBkdmWOU4c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "row_sums = masked_simple.sum(dim=-1, keepdim=True)\n",
        "masked_simple_norm = masked_simple / row_sums\n",
        "print(masked_simple_norm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oTz-zMoXNIj1",
        "outputId": "83ca134e-68f5-449c-be53-1eb54538dcfe"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\n",
            "        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\n",
            "        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],\n",
            "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
            "       grad_fn=<DivBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Information Leakage**\n",
        "\n",
        "When we apply a mask and then renormalize the attention weights, it might initialy appear that information from future tokens (which we intend to mask) could still influence the current token because their values are part of the softmax calculation. However, the key insight is that when we renormalize the attention weights after masking, what we're essentially doing is recalculating the softmax over a smaller subset (since masked positions don't contribute to softmax value).\n",
        "\n",
        "The mathematical elegance of softmax is that despite initially including all positions in the denominator, after masking and renormalizing, the effect of the masked positions is nullified, they don't contribute to the softmax score in any meaningful way.\n",
        "\n",
        "In simpler terms, after masking and renormalization, the distribution of attention weights is as if it was calculated only among the unmasked positions to begin with. This ensures there's no information leakage from future (or otherwise masked) tokens as we intended."
      ],
      "metadata": {
        "id": "WPLr2TGTO-9V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "While we could wrap up our implementation of causal attention at this point, we can still improve it. Let's take a mathematical property of the softmax function and implement the computation of the masked attention weights more efficiently in fewer steps:\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1AOpNiUJm9itZxwfK7rTJK26NPUBYZb8c\">\n",
        "\n",
        "The softmax function converts its inputs into a probability distribution. When negative infinity values (-∞) are present in a row, the softmax function treats them as zero probability. Mathematically, this is because e^-∞ approaches 0.\n",
        "\n",
        "We can implement this more efficient masking \"trick\" by creating a mask with 1s above the diagonal and then replacing these 1s with negative infinity (-inf) values:"
      ],
      "metadata": {
        "id": "hKyra0mLQOG1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mask = torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
        "print(mask)\n",
        "masked = attn_scores.masked_fill(mask.bool(), -torch.inf)\n",
        "print(masked)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Weo9mTVONJhY",
        "outputId": "a17d3add-3fc1-46fd-f08c-bb2ae2483b82"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0., 1., 1., 1., 1., 1.],\n",
            "        [0., 0., 1., 1., 1., 1.],\n",
            "        [0., 0., 0., 1., 1., 1.],\n",
            "        [0., 0., 0., 0., 1., 1.],\n",
            "        [0., 0., 0., 0., 0., 1.],\n",
            "        [0., 0., 0., 0., 0., 0.]])\n",
            "tensor([[0.2899,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
            "        [0.4656, 0.1723,   -inf,   -inf,   -inf,   -inf],\n",
            "        [0.4594, 0.1703, 0.1731,   -inf,   -inf,   -inf],\n",
            "        [0.2642, 0.1024, 0.1036, 0.0186,   -inf,   -inf],\n",
            "        [0.2183, 0.0874, 0.0882, 0.0177, 0.0786,   -inf],\n",
            "        [0.3408, 0.1270, 0.1290, 0.0198, 0.1290, 0.0078]],\n",
            "       grad_fn=<MaskedFillBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now all we need to do is apply the softmax function to these masked results, and we are done"
      ],
      "metadata": {
        "id": "YCyEtBoFTGj8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "attn_weights = torch.softmax(masked / keys.shape[-1]**0.5, dim=-1)\n",
        "print(attn_weights)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FxJAAGZINKnp",
        "outputId": "1640a104-7c30-4939-bda9-18ef989fd368"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\n",
            "        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\n",
            "        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],\n",
            "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
            "       grad_fn=<SoftmaxBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We could now use the modified attention weights to compute the context vectors via context_vec = attn_weights @ values, as in section 3.4. However, we will first cover another minor tweak to the causal attention mechanism that is useful for reducing overfitting when training LLMs."
      ],
      "metadata": {
        "id": "QlJhKNMKTU-0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3.5.2 Masking Additional Attention Weights with Dropout**"
      ],
      "metadata": {
        "id": "CuxdrBv0UYqf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dropout in deep learning is a technique where randomly selected hidden layer units are ignored during training, effectively \"dropping\" them out. This method helps prevent overfitting by ensuring that a model does not become overly reliant on any specific set of hidden layer units. It's important to emphasize that dropout is only used during training and is disabled afterward.\n",
        "\n",
        "In the transformer architecture, including models like GPT, dropout in the attention mechanism is typically applied at two specific times: after calculating the attention weights or after applying the attention weights to the value vectors. Here we will apply the dropout mask after computing the attention weights, as illustrated in below figure, because it's more common variant in practice.\n",
        "\n",
        "In the following code example, we use a dropout rate of 50%, which means masking out half of the attention weights. (When we train the GPT model in later chapters, we will use a lower dropout rate, such as 0.1 or 0.2). We apply PyTorch's dropout implementation first to a 6x6 tensor consisting of 1s for simplicity:\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1hV5u2C3mUn95jnhBhMHVycpzPRd7eYaI\">"
      ],
      "metadata": {
        "id": "nOWh--UEUden"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "dropout = torch.nn.Dropout(0.5) # dropout rate of 50%\n",
        "example = torch.ones(6, 6) # create a matrix of ones\n",
        "\n",
        "print(dropout(example))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RXhHJhH2YAY7",
        "outputId": "dc6d08f6-5c94-45cf-db16-7a4b43b0f916"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[2., 2., 2., 2., 2., 2.],\n",
            "        [0., 2., 0., 0., 0., 0.],\n",
            "        [0., 0., 2., 0., 2., 0.],\n",
            "        [2., 2., 0., 0., 0., 2.],\n",
            "        [2., 0., 0., 0., 0., 2.],\n",
            "        [0., 2., 0., 0., 0., 0.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "When applying dropout to an attention weight matrix with a rate of 50%, half of the elements in the matrix are randomly set to zero. To compensate for the reduction in active elements, the values of the remaining elements in the matrix are scaled up by a factor of 1/0.5 = 2. This scaling is crucial to maintain the overall balance of the attention weights, ensuring that the average influence of the attention mechanism remains consistent during both the training and inference phases.\n",
        "\n",
        "Now let's apply dropout to the attention weight matrix itself:"
      ],
      "metadata": {
        "id": "OquAvWkBYHUi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "print(dropout(attn_weights))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dPkdeHVQYFAE",
        "outputId": "c11cb271-e39c-45c7-82fb-6f1a4a85bed3"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[2.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.0000, 0.8966, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.0000, 0.0000, 0.6206, 0.0000, 0.0000, 0.0000],\n",
            "        [0.5517, 0.4921, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.4350, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.0000, 0.3327, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
            "       grad_fn=<MulBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The resulting attention weight matrix now has additional elements zeroed out and the remaining 1s rescaled\n",
        "\n",
        "Note that the resulting dropout outputs may look different depending on your operating system.\n",
        "\n",
        "Having gained an understanding of causal attention and dropout masking, we can now develop a concise Python class. This class is designed to facilitate the efficient application of these two techniques."
      ],
      "metadata": {
        "id": "LcAacAHRZlcl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3.5.3 Implementing a Compact Causal Attention Class**\n",
        "\n",
        "We will now incorporate the causal attention and dropout modifications into the SelfAttention Python class we developed in section 3.4. This class will then serve as a template for developing multi-head attention, which is the final attention class we will implement.\n",
        "\n",
        "But before we begin, let's ensure that the code can handle batches consisting of more than one input so that the CausalAttention class supports the batch outputs produced by the data loader we implemented in chapter 2.\n",
        "\n",
        "For simplicity, to simulate such batch inputs, we duplicate the input text example:"
      ],
      "metadata": {
        "id": "vo98T6O1aE5F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch = torch.stack((inputs, inputs), dim=0)\n",
        "print(batch)\n",
        "print(batch.shape) # 2 inputs with 6 tokens each, and each token has embedding dimension 3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wGcpN2nOa3xx",
        "outputId": "d6df3b26-0583-47e8-b86f-04112754ac69"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[0.4300, 0.1500, 0.8900],\n",
            "         [0.5500, 0.8700, 0.6600],\n",
            "         [0.5700, 0.8500, 0.6400],\n",
            "         [0.2200, 0.5800, 0.3300],\n",
            "         [0.7700, 0.2500, 0.1000],\n",
            "         [0.0500, 0.8000, 0.5500]],\n",
            "\n",
            "        [[0.4300, 0.1500, 0.8900],\n",
            "         [0.5500, 0.8700, 0.6600],\n",
            "         [0.5700, 0.8500, 0.6400],\n",
            "         [0.2200, 0.5800, 0.3300],\n",
            "         [0.7700, 0.2500, 0.1000],\n",
            "         [0.0500, 0.8000, 0.5500]]])\n",
            "torch.Size([2, 6, 3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This result in a three-dimensional tensor consisting of two input texts with six tokens each, where each token is a three-dimensional embedding vector."
      ],
      "metadata": {
        "id": "8u4SRRAhbH9k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The CausalAttention class is similar to SelfAttention class we implemented earlier, with addition of masking and dropout."
      ],
      "metadata": {
        "id": "C8Y5dWaybRvl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CausalAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, d_in, d_out, context_length,\n",
        "                 dropout, qkv_bias=False):\n",
        "        super().__init__()\n",
        "        self.d_out = d_out\n",
        "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.dropout = nn.Dropout(dropout) # New\n",
        "        self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal=1)) # New\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, num_tokens, d_in = x.shape # New batch dimension b\n",
        "        # For inputs where `num_tokens` exceeds `context_length`, this will result in errors\n",
        "        # in the mask creation further below.\n",
        "        # In practice, this is not a problem since the LLM (chapters 4-7) ensures that inputs\n",
        "        # do not exceed `context_length` before reaching this forward method.\n",
        "        keys = self.W_key(x)\n",
        "        queries = self.W_query(x)\n",
        "        values = self.W_value(x)\n",
        "\n",
        "        attn_scores = queries @ keys.transpose(1, 2) # Changed transpose\n",
        "        attn_scores.masked_fill_(  # New, _ ops are in-place\n",
        "            self.mask.bool()[:num_tokens, :num_tokens], -torch.inf)  # `:num_tokens` to account for cases where the number of tokens in the batch is smaller than the supported context_size\n",
        "        attn_weights = torch.softmax(\n",
        "            attn_scores / keys.shape[-1]**0.5, dim=-1\n",
        "        )\n",
        "        attn_weights = self.dropout(attn_weights) # New\n",
        "\n",
        "        context_vec = attn_weights @ values\n",
        "        return context_vec"
      ],
      "metadata": {
        "id": "60fCCtOha7ov"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "While all added code lines should be familiar at this point, we now added a self.register_buffer() call in the init method. The use of register_buffer in PyTorch is not strictly necessary for all use cases but offers several advantages here. For instance, when we use the CausalAttention class in our LLM, buffers are automatically moved to the appropriate device (CPU or GPU) along with our model, which will be relevant when training our LLM. This means we don't need to manually ensure these tensors are on the same device as your model parameters, avoiding device mismatch errors."
      ],
      "metadata": {
        "id": "kfG2-NINbwIk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can use the CausalAttention class as follows, similar to SelfAttention previously:"
      ],
      "metadata": {
        "id": "5U2jXqyKc3Ri"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "\n",
        "context_length = batch.shape[1]\n",
        "ca = CausalAttention(d_in, d_out, context_length, 0.0)\n",
        "\n",
        "context_vecs = ca(batch)\n",
        "\n",
        "print(context_vecs)\n",
        "print(\"context_vecs.shape:\", context_vecs.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "0Szb1jmWc_cb",
        "outputId": "ade8dff0-b66c-441a-b5a5-6bbdaeed76bc"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "mat1 and mat2 shapes cannot be multiplied (12x3 and 768x768)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-49-3252603583.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mca\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCausalAttention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mcontext_vecs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mca\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext_vecs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-48-2482187013.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;31m# In practice, this is not a problem since the LLM (chapters 4-7) ensures that inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;31m# do not exceed `context_length` before reaching this forward method.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mkeys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mqueries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW_query\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (12x3 and 768x768)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The resulting context vector is a three-dimensional tensor where each token is now represented by a two-dimensional embedding\n",
        "\n",
        "<figure>\n",
        "<img src=\"https://drive.google.com/uc?id=1t_CIbU6ZiG2mQxv-eKVlKp1mHPYMlVQk\">\n",
        "<figcaption>Figure 3.5.3</figcaption>\n",
        "</figure>"
      ],
      "metadata": {
        "id": "UXap6DmodBfY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3.6 Extending Single-head Attention to Multi-head Attention**"
      ],
      "metadata": {
        "id": "-H1w539xjIgD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our final step will be to extend the previously implemented causal attention class over multiple heads. This is also called multi-head attention.\n",
        "\n",
        "The term \"multi-head\" refers to dividing the attention mechanism into multiple \"heads\", each operating independently. In this context, a single causal attention module can be considered single-head attention, where there is only one set of attention weights processing the input sequentially.\n",
        "\n",
        "We will tackle this expansion from causal attention to multi-head attention. First, we will intuitively build a multi-head attention module by stacking multiple CausalAttention modules. Then we will then implement the same multi-head attention module in a more complicated but more computationally efficent way."
      ],
      "metadata": {
        "id": "OCqDP_P7jRmG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3.6.1 Stacking Multiple Single-head Attention Layers**"
      ],
      "metadata": {
        "id": "jJiC6nAskOz7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "In practical terms, implementing multi-head attention involves creating multiple instances of the self-attention mechanism, each with its own weights, and then combining their outputs. Using multiple instances of the self-attention mechanism can be computationally intensive, but it's crucial for the kind of complex pattern recognition that models like transformer-based LLMs are known for.\n",
        "\n",
        "Figure below illustrates the structure of a multi-head attention module, which consists of multiple single-head attention modules, as previously depicted in figure 3.4.2.1, stacked on top of each other.\n",
        "\n",
        "<figure>\n",
        "<img src=\"https://drive.google.com/uc?id=1hv-0kT4KA9LBBSkjKpBCi5PbgDHe8IFI\">\n",
        "<figcaption>Figure 3.6.1.1</figcaption>\n",
        "</figure>\n",
        "\n",
        "The multi-head attention module includes two single-head attention modules stacked on top of each other. So, instead of using a single matrix Wv for computing the values matrices, in a multi-head attention module with two heads, we now have two value weight matrice: Wv1 and Wv2. The same applies to the other weight matrices, Wq and Wk. We obtain two sets of context vectors Z1 and Z2 that we can combine into a single context vector matrix Z.\n",
        "\n",
        "As mentioned before, the main idea behind multi-head attention is to run the attention mechanism multiple times (in parallel) with different, learned linear projections, the results of multiplying the input data (like the query, key, and value vectors in attention mechanisms) by a weight matrix. In code, we can achieve this by implementing a simple MultiHeadAttentionWrapper class that stacks multiple instances of our previously implemented CausalAttention module."
      ],
      "metadata": {
        "id": "-qcRMqGPkUL6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttentionWrapper(nn.Module):\n",
        "\n",
        "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList(\n",
        "            [CausalAttention(d_in, d_out, context_length, dropout, qkv_bias)\n",
        "             for _ in range(num_heads)]\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return torch.cat([head(x) for head in self.heads], dim=-1)\n"
      ],
      "metadata": {
        "id": "B-McNcU1sekV"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For example, if we use this MultiHeadAttentionWrapper class with two attention heads (via num_heads=2) and CausalAttention output dimension d_out=2, we get a four-dimensional context vector (d_out*num_heads=4), as depicted in figure 3.6.1.2:\n",
        "\n",
        "\n",
        "<figure>\n",
        "<img src=\"https://drive.google.com/uc?id=17l3iN_VaWiul6Wn-iO-jEY_FlcTB-YHG\">\n",
        "<figcaption>Figure 3.6.1.2</figcaption>\n",
        "</figure>\n"
      ],
      "metadata": {
        "id": "CccUZFHPs_Kv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To illustrate this further with a concrete example, we can use the MultiHeadAttentionWrapper class similar to the CausalAttention class before:"
      ],
      "metadata": {
        "id": "qEHfs5FcvjkL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "\n",
        "context_length = batch.shape[1] # This is the number of tokens\n",
        "d_in, d_out = 3, 2\n",
        "mha = MultiHeadAttentionWrapper(\n",
        "    d_in, d_out, context_length, 0.0, num_heads=2\n",
        ")\n",
        "\n",
        "context_vecs = mha(batch)\n",
        "\n",
        "print(context_vecs)\n",
        "print(\"context_vecs.shape:\", context_vecs.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Be0QHPXviNm",
        "outputId": "7e8611cf-5ef2-4d81-dcd6-8a06c29e7c1a"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[-0.4519,  0.2216,  0.4772,  0.1063],\n",
            "         [-0.5874,  0.0058,  0.5891,  0.3257],\n",
            "         [-0.6300, -0.0632,  0.6202,  0.3860],\n",
            "         [-0.5675, -0.0843,  0.5478,  0.3589],\n",
            "         [-0.5526, -0.0981,  0.5321,  0.3428],\n",
            "         [-0.5299, -0.1081,  0.5077,  0.3493]],\n",
            "\n",
            "        [[-0.4519,  0.2216,  0.4772,  0.1063],\n",
            "         [-0.5874,  0.0058,  0.5891,  0.3257],\n",
            "         [-0.6300, -0.0632,  0.6202,  0.3860],\n",
            "         [-0.5675, -0.0843,  0.5478,  0.3589],\n",
            "         [-0.5526, -0.0981,  0.5321,  0.3428],\n",
            "         [-0.5299, -0.1081,  0.5077,  0.3493]]], grad_fn=<CatBackward0>)\n",
            "context_vecs.shape: torch.Size([2, 6, 4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first dimension of the resulting context_vectors is 2 since we have two input texts (the input texts are duplicated, which is why the context vectors are exactly the same for those). The second dimension refers to the 6 tokens in each input. The third dimension refers to the four-dimension embedding of each token."
      ],
      "metadata": {
        "id": "rqHcP93Cvwb-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise**\n",
        "\n",
        "Change the input arguments for the MultiHeadAttentionWrapper call such that the output context vectors are two-dimensional instead of four dimensional while keeping the setting num_heads=2."
      ],
      "metadata": {
        "id": "uK3Q5qxuwr0W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "\n",
        "context_length = batch.shape[1] # This is the number of tokens\n",
        "d_in, d_out = 3, 1 # <--- just change the d_out to 1 and the context vector should be 2 dimensional which is the result of\n",
        "                   # merging 2 one dimensional context vector (num_heads=2)\n",
        "mha = MultiHeadAttentionWrapper(\n",
        "    d_in, d_out, context_length, 0.0, num_heads=2\n",
        ")\n",
        "\n",
        "context_vecs = mha(batch)\n",
        "\n",
        "print(context_vecs)\n",
        "print(\"context_vecs.shape:\", context_vecs.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YkgCRKIhwqJI",
        "outputId": "8e49f424-ca00-46ad-9e8d-98d4913c3c42"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[-0.5740,  0.2216],\n",
            "         [-0.7320,  0.0155],\n",
            "         [-0.7774, -0.0546],\n",
            "         [-0.6979, -0.0817],\n",
            "         [-0.6538, -0.0957],\n",
            "         [-0.6424, -0.1065]],\n",
            "\n",
            "        [[-0.5740,  0.2216],\n",
            "         [-0.7320,  0.0155],\n",
            "         [-0.7774, -0.0546],\n",
            "         [-0.6979, -0.0817],\n",
            "         [-0.6538, -0.0957],\n",
            "         [-0.6424, -0.1065]]], grad_fn=<CatBackward0>)\n",
            "context_vecs.shape: torch.Size([2, 6, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Up to this point, we have implemented a MultiHeadAttentionWrapper that combined multiple single-head attention modules. However, these are processed sequentially via [head(x) for head in self.heads] in the forward method. We can improve this implementation by processing the heads in parallel. One way to achieve this is by computing the outputs for all attention heads simultaneously via matrix multiplication."
      ],
      "metadata": {
        "id": "E9hl7SYkxUAA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3.6.2 Implementing Multi-head Attention with Weight Splits**"
      ],
      "metadata": {
        "id": "XKLTzNL5x4DL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "So far, we have created a MultiHeadAttentionWrapper to implement multi-head attention by stacking multiple single-head attention modules. This was done by instantiating and combining several CausalAttention objects.\n",
        "\n",
        "Instead of maintaining two separate classes, MultiHeadAttentionWrapper and CausalAttention, we can combine these concepts into a single MultiHeadAttention class. Also, in addition to merging those two function, we will make modifications to implement multi-head attention more efficiently.\n",
        "\n",
        "In MultiHeadAttentionWrapper, multiple heads are implemented by creating a list of CausalAttention objects (self.heads), each representing a separate attention head. The CausalAttention class independently performs the attention mechanism, and the results from each head are concatenated. In contrast, the following MultiHeadAttention class integrates the multi-head functionality within a single class. It splits the input into multiple heads by reshaping the projected query, key, and value tensors and then combines the results from these heads after computing attention."
      ],
      "metadata": {
        "id": "UJdGykl3yGW6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
        "        super().__init__()\n",
        "        assert (d_out % num_heads == 0), \\\n",
        "            \"d_out must be divisible by num_heads\"\n",
        "\n",
        "        self.d_out = d_out\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = d_out // num_heads # Reduce the projection dim to match desired output dim\n",
        "\n",
        "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.register_buffer(\n",
        "            \"mask\",\n",
        "            torch.triu(torch.ones(context_length, context_length),\n",
        "                       diagonal=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, num_tokens, d_in = x.shape\n",
        "        # As in `CausalAttention`, for inputs where `num_tokens` exceeds `context_length`,\n",
        "        # this will result in errors in the mask creation further below.\n",
        "        # In practice, this is not a problem since the LLM (chapters 4-7) ensures that inputs\n",
        "        # do not exceed `context_length` before reaching this forwar\n",
        "\n",
        "        keys = self.W_key(x) # Shape: (b, num_tokens, d_out)\n",
        "        queries = self.W_query(x)\n",
        "        values = self.W_value(x)\n",
        "\n",
        "        # We implicitly split the matrix by adding a `num_heads` dimension\n",
        "        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n",
        "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "\n",
        "        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n",
        "        keys = keys.transpose(1, 2)\n",
        "        queries = queries.transpose(1, 2)\n",
        "        values = values.transpose(1, 2)\n",
        "\n",
        "        # Compute scaled dot-product attention (aka self-attention) with a causal mask\n",
        "        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n",
        "\n",
        "        # Original mask truncated to the number of tokens and converted to boolean\n",
        "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
        "\n",
        "        # Use the mask to fill attention scores\n",
        "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
        "\n",
        "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
        "        attn_weights = self.dropout(attn_weights)\n",
        "\n",
        "        # Shape: (b, num_tokens, num_heads, head_dim)\n",
        "        context_vec = (attn_weights @ values).transpose(1, 2)\n",
        "\n",
        "        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n",
        "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
        "        context_vec = self.out_proj(context_vec) # optional projection\n",
        "\n",
        "        return context_vec"
      ],
      "metadata": {
        "id": "_gOcQwGuzbNp"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Even though the reshaping (.view) and transposing (.transpose) of tensors inside the MultiHeadAttention class looks very mathematically complicated, the MultiHeadAttention class implements the same concept as the MultiHeadAttentionWrapper earlier.\n",
        "\n",
        "On a big-picture level, in the previous MultiHeadAttentionWrapper, we stacked multiple single-head attention layers that we combined into a multi-head attention layer. The MultiHeadAttention class takes an integrated approach. It starts with a multi-head layer and then internally splits this layer into individual attention heads, as illustrated in figure 3.6.2.\n",
        "\n",
        "The splitting of the query, key, and value tensors is achieved through tensor reshaping and transposing operations using PyTorch's .view and .transpose methods. The input is first transformed (via linear layers for queries, keys, and values) and then reshaped to represent multiple heads.\n",
        "\n",
        "The key operation is to split the d_out dimension into num_heads and head_dim, where head_dim = d_out/num_heads. This splitting is then achieved using the .view method: a tensor of dimensions (b, num_tokens, d_out) is reshaped to dimension (b, num_tokenes, num_heads, head_dim)."
      ],
      "metadata": {
        "id": "YQwEVdwbAbWG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<figure>\n",
        "<img src=\"https://drive.google.com/uc?id=1n0aKo-xoGrr98y8Sl80Wk-I7Qnv7XE_J\">\n",
        "<figcaption>Figure 3.6.2</figcaption>\n",
        "</figure>\n",
        "\n",
        "Top is using MultiHeadAttentionWrapper, bottom is using MultiHeadAttention."
      ],
      "metadata": {
        "id": "gODySA5p2SWp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The tensors are then transposed to bring the num_heads dimension before the num_tokens dimension, resulting in a shape of (b, num_heads, num_tokens, head_dim). This transposition is crucial for correctly aligning the queries, keys, and values accross the different heads and performing batched matrix multiplications efficiently. To illustrate this batched matrix multiplication, suppose we have the following tensor:"
      ],
      "metadata": {
        "id": "Cnz-AgQ9X87_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# (b, num_heads, num_tokens, head_dim) = (1, 2, 3, 4)\n",
        "a = torch.tensor([[[[0.2745, 0.6584, 0.2775, 0.8573], # the shape of this tensor is (b, num_heads, num_tokens, head_dim) = (1,2,3,4)\n",
        "                    [0.8993, 0.0390, 0.9268, 0.7388],\n",
        "                    [0.7179, 0.7058, 0.9156, 0.4340]],\n",
        "\n",
        "                   [[0.0772, 0.3565, 0.1479, 0.5331],\n",
        "                    [0.4066, 0.2318, 0.4545, 0.9737],\n",
        "                    [0.4606, 0.5159, 0.4220, 0.5786]]]])"
      ],
      "metadata": {
        "id": "rWaWGhrJzrTs"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we perform a batched matrix multiplication between the tensor itself and a view of the tensor where we transposed the last two dimensions, num_tokens and head_dim"
      ],
      "metadata": {
        "id": "yojxgtJG8Fqk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(a @ a.transpose(2, 3)) # This is a dot product"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mv5cEbHq8VnE",
        "outputId": "e9f80c60-7030-4721-8876-07df135fd29b"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[[1.3208, 1.1631, 1.2879],\n",
            "          [1.1631, 2.2150, 1.8424],\n",
            "          [1.2879, 1.8424, 2.0402]],\n",
            "\n",
            "         [[0.4391, 0.7003, 0.5903],\n",
            "          [0.7003, 1.3737, 1.0620],\n",
            "          [0.5903, 1.0620, 0.9912]]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this case, the matrix multiplication implementation in PyTorch handles the four-dimensional input tensor so that the matrix multiplication is carried out between the two last dimensions (num_tokens, head_dim) and then repeated for the individual heads.\n",
        "\n",
        "For instance, the preceding becomes a more compact way to compute the matrix multiplication for each head separately:"
      ],
      "metadata": {
        "id": "bx1Jy8no8nU-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "first_head = a[0, 0, :, :]\n",
        "first_res = first_head @ first_head.T\n",
        "print(\"First head:\\n\", first_res)\n",
        "\n",
        "second_head = a[0, 1, :, :]\n",
        "second_res = second_head @ second_head.T\n",
        "print(\"\\nSecond head:\\n\", second_res)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bANTDOZTzruh",
        "outputId": "3f06d74e-97d8-4b76-8cf1-bc0d1c2c5022"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First head:\n",
            " tensor([[1.3208, 1.1631, 1.2879],\n",
            "        [1.1631, 2.2150, 1.8424],\n",
            "        [1.2879, 1.8424, 2.0402]])\n",
            "\n",
            "Second head:\n",
            " tensor([[0.4391, 0.7003, 0.5903],\n",
            "        [0.7003, 1.3737, 1.0620],\n",
            "        [0.5903, 1.0620, 0.9912]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The results are exactly the same results as those we obtained when using the batched matrix multiplication print(a @ a.transpose(2,3))"
      ],
      "metadata": {
        "id": "FiARxY1E9ZJd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Continuing with MultiHeadAttention, after computing the attention weights and context vectors, the context vectors from all heads are transposed back to the shape (b, num_tokens, num_heads, d_out), effectively combining the outputs from all heads.\n",
        "\n",
        "Additionally, we added an output projection layer (self.out_proj) to MultiHeadAttention after combining the heads, which is not present in the CausalAttention class. This output projection layer is not strictly necessary, but it is commonly used in many LLM architectures.\n",
        "\n",
        "Even though the MultiHeadAttention class looks more complicated than the MultiHeadAttentionWrapper due to the additional reshaping and transposition of tensors, it is more efficient. The reason is that we only need one matrix multiplication to compute the keys, for instance, keys = self.W_key(x) (and value and queries too). In the MultiHeadAttentionWrapper, we needed to repeat this matrix multiplication, which is computationally one of the most expensive steps, for each attention head."
      ],
      "metadata": {
        "id": "NfPMo1bB_z1S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "\n",
        "batch_size, context_length, d_in = batch.shape\n",
        "d_out = 2\n",
        "mha = MultiHeadAttention(d_in, d_out, context_length, 0.0, num_heads=2)\n",
        "\n",
        "context_vecs = mha(batch)\n",
        "\n",
        "print(context_vecs)\n",
        "print(\"context_vecs.shape:\", context_vecs.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F6-wP-hLzmtL",
        "outputId": "1978a52b-468f-4e14-9550-e7d7de4da9cf"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[0.3190, 0.4858],\n",
            "         [0.2943, 0.3897],\n",
            "         [0.2856, 0.3593],\n",
            "         [0.2693, 0.3873],\n",
            "         [0.2639, 0.3928],\n",
            "         [0.2575, 0.4028]],\n",
            "\n",
            "        [[0.3190, 0.4858],\n",
            "         [0.2943, 0.3897],\n",
            "         [0.2856, 0.3593],\n",
            "         [0.2693, 0.3873],\n",
            "         [0.2639, 0.3928],\n",
            "         [0.2575, 0.4028]]], grad_fn=<ViewBackward0>)\n",
            "context_vecs.shape: torch.Size([2, 6, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The results show that the output dimension is directly controlled by the d_out argument."
      ],
      "metadata": {
        "id": "zQzlbcVnCBqg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have now implemented the MultiHeadAttention class that we will use when we implement and train the LLM. Note that while the code is fully functional, we used relatively small embedding sizes and numbers of attention heads to keep the outputs readable.\n",
        "\n",
        "Smallest GPT-2 has 117 million parameters, 12 attention heads, and a context vector embedding size of 768, while the largest GPT-2 has 1.5 billion parameters, 25 attention heads, and a context vector embedding size of 1600. The embedding size of the token inputs and context embeddings are the same in GPT models (d_in=d_out)"
      ],
      "metadata": {
        "id": "ZQKon2k-CWnB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise**\n",
        "\n",
        "Using the MultiHeadAttention class, initialize a multi-head attention module that has the same number of attention heads as the smallest GPT-2 model (12 attention heads). Also ensure that you use the respective input and output embedding sizes similar to GPT-2 (768 dimensions). None that the smallest GPT-2 model supports a context length of 1024 tokens."
      ],
      "metadata": {
        "id": "tXeO1OfuDk5g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "d_in= 768\n",
        "d_out = 768 #Since in GPT-2 embedding sizes of each token input and context embeddings are the same (d_in=d_out),\n",
        "# then from the output embedding size of 768 dimension, we can initialize the d_in = 768\n",
        "context_length = 1024\n",
        "\n",
        "mha = MultiHeadAttention(d_in, d_out, context_length, 0.0, num_heads=12) # 12 attention heads mean num_heads=12\n",
        "print(mha)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_hOrJSFuDihT",
        "outputId": "a641d35c-9a95-41fc-cb5c-5cbc32b47a76"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MultiHeadAttention(\n",
            "  (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
            "  (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
            "  (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
            "  (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (dropout): Dropout(p=0.0, inplace=False)\n",
            ")\n"
          ]
        }
      ]
    }
  ]
}