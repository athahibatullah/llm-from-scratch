{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMTigQXWHljAQ61I92vYdSB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/athahibatullah/llm-from-scratch/blob/main/03%20-%20Coding%20Attention%20Mechanisms/ch03.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Coding Attention Mechanisms**"
      ],
      "metadata": {
        "id": "Mm8N88xy-P5g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we will lok at an integral part of the LLM architecture itself, attention mechanisms.\n",
        "\n",
        "Figure below depicts different attention mechanisms we will code in this chapter. These different attention variants build on each other, and the goal is to arrive at a compact and efficient implementation of multi-head attention that we can then plug into the LLM architecture we will code in the next chapter.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1eY2OWPT3QfDvouOPbSP0YfV4_PzfF7EG\">"
      ],
      "metadata": {
        "id": "j3dU9277-HMo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3.1 The Problem with Modeling Long Sequences**"
      ],
      "metadata": {
        "id": "7ct9xjmu_L4k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before we dive into self-attention mechanism, we should know the problem with pre-LLM architectures that do not include attention mechanisms.\n",
        "\n",
        "Suppose we want to build a language translation model, we can't translate a text from a language to another language in the same sequence because grammar exist\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1WxOhVuKOqN-nR_RWcI8Z6E-5CscsQt0p\">\n",
        "\n",
        "To address this problem, it is common to use a deep neural network with two submodules, an encoder and a decoder. The job of the encoder is to first read in and process the entire text, and the decoder then produces the translated text.\n",
        "\n",
        "Before the advent of transformers, recurrent neural networks (RNN) were the most popular encoder-decoder architecture for language translation. An RNN is a type of neural network where outputs from previous steps are fed as inputs to the current step, making them well-suited for sequential data like text.\n",
        "\n",
        "In encoder-decoder RNN, the input text is fed into the encoder, which processes it sequentially. The encoder updates its hidden state (the internal values at the hidden layers) at each step, tryning to capture the entire meaning of the input sentence in the final hidden state. The decoder then start to take this final hidden state to start translating text one word at a time. It also updates its hidden state at each step, which is supposed to carry the context necessary for the next-word prediction.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1ouD6Ma4e4ScqF3Sr5UW7cJWsKDWnkXWj\">\n",
        "\n",
        "The key idea here is that encoder part processes the entire input text into a hidden state (memory cell). The decoder then takes in the hidden state to produce the output. Hidden state can be analogize like embedding vector.\n",
        "\n",
        "The big limitation of encoder-decoder RNNs is that the RNN can't directly access earlier hidden states from the encoder during the decoding phase. It relies solely on the current hidden state, which encapsulates all relevant information. This can lead to a loss of context, especially in complex sentences where dependencies might span long distances. It is not essential to understand RNNs to build an LLM, but the limitation of the RNNs is the motivation behind the design of attention mechanisms."
      ],
      "metadata": {
        "id": "dei4frA8_SaQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3.2 Capturing Data Dependencies with Attention Mechanisms**"
      ],
      "metadata": {
        "id": "sekzCCZBQSss"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Although RNNs work fine for translating short sentences, they don't work well for longer texts as they don't have direct access to previous words in the input. One major shortcoming in this approach is that the RNN must remember the entire encoded input in a single hidden state before passing it to the decoder.\n",
        "\n",
        "Hence, researchers developed the Bahdanau attention mechanism for RNNs in 2014, which modifies the encoder-decoder RNN such that the encoder can selectively access different parts of the input sequence at each decoding step:\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1XB4eRzWnkYccOMgoiuUIcTeXfHu49gWI\">\n",
        "\n",
        "From the image, the text-generating decoder part of the network can access all input tokens selectively. This means that some input tokens are more important than others for generating a given output token. The importance is determined by the attention weights"
      ],
      "metadata": {
        "id": "AwBqKZe6QXf3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3 years later, researchers found that RNN architectures are not required for building deep neural networks for natural language processing and proposed the original transformer architecture including a self-attention mechanism inspired by the Bahdanau attention mechanism.\n",
        "\n",
        "Self-attention is a mechanism that allows each position in the input sequence to consider the relevancy of, or \"attend to,\" all other positions in the same sequence when computing the representation of a sequence. Self-attention is a key component to LLMs based on the transformer architecture.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1eOc8sBHNafdxp-rzQpGttDBvqXDaB2kl\">"
      ],
      "metadata": {
        "id": "JZ22QG_YXmQm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3.3 Attending to Different Parts of the Input with Self-Attention**"
      ],
      "metadata": {
        "id": "DryhY5aoarAA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The \"self\" in self-attention refers to the mechanism's ability to compute attention weights by relating different positions within a single input sequence. It assess and learns the relationships and dependencies between various parts of the input itself, such as words in a sentence or pixels in an image.\n",
        "\n",
        "This is contrast to traditional attention mechanisms, where the focus is on the relationships between elements of two different sequences, such as in sequence-to-sequence models where the attention might be between an input sequence and an output sequence."
      ],
      "metadata": {
        "id": "eRMH2a8kaxcu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3.3.1 A Simple Self-Attention Mechanism without Trainable Weights**"
      ],
      "metadata": {
        "id": "i5Aj1wTjc1K_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's begin by implementing a simplified variant of self-attention, free from any trainable weights, as summarized in figure below. The goal is to illustrate a few key concepts in self-attention before adding trainable weights.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1EzLfE00OGTZmrB2N1-aBu-OKVCMTcTkJ\">\n",
        "\n",
        "Figure above shows an input sequence, denoted as x, consisting of T elements represented as x1 to xT. This sequence typically represents text, such as sentence, that has already been transformed into token embeddings."
      ],
      "metadata": {
        "id": "Pw7LI4oTc8No"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In self-attention, our goal is to calculate context vectors zi for each element xi in the input sequence. A context vector can be interpreted as an enriched embedding vector.\n",
        "\n",
        "To illustrate this concept by using above figure, let's focus on the embedding vector of the second input element, x2 (\"journey\"), and the corresponding context vector, z2. This enhanced context vector, z2, is an embedding that contains information about x2 and all other input elements, x1 to xT.\n",
        "\n",
        "Context vectors play a crucial role in self-attention. Their purpose is to create enriched representations of each element in an input sequence (like a sentence) by incorporating information from all other elements in the sequence. This is essential for LLM since LLM require relationship and relevance of words to understand context. We will add trainable weights that help an LLM learn to construct these context vectors. For now, let's implement simplified self-attention mechanisms to compute these weights and the resulting context vector one step at a time."
      ],
      "metadata": {
        "id": "MS6gMuYOikf9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Consider the following input sentence, which has already been embedded into three-dimensional vectors."
      ],
      "metadata": {
        "id": "TzN01jqE0NU6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "inputs = torch.tensor(\n",
        "  [[0.43, 0.15, 0.89], # Your     (x^1)\n",
        "   [0.55, 0.87, 0.66], # journey  (x^2)\n",
        "   [0.57, 0.85, 0.64], # starts   (x^3)\n",
        "   [0.22, 0.58, 0.33], # with     (x^4)\n",
        "   [0.77, 0.25, 0.10], # one      (x^5)\n",
        "   [0.05, 0.80, 0.55]] # step     (x^6)\n",
        ")"
      ],
      "metadata": {
        "id": "tw8AzUtXzlqK"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first step of implementing self-attention is to compute the intermediate values ω, referred to as attention scores (figure below). Due to spatial constraints, the figure displayed the values in truncated version. For example 0.87 is truncated to 0.8.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1ZNs_Nw-pfGhBtfqbrsUlSfitAWtZj4-3\">"
      ],
      "metadata": {
        "id": "Yn4hSUng0etE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From figure above (with truncated values), we can determine attention scores between the query token and each input token by computing the dot product of the query, x2, with every other input token:"
      ],
      "metadata": {
        "id": "LlrjpfbI3uiZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = inputs[1]  # 2nd input token is the query\n",
        "\n",
        "attn_scores_2 = torch.empty(inputs.shape[0])\n",
        "for i, x_i in enumerate(inputs):\n",
        "    attn_scores_2[i] = torch.dot(x_i, query) # dot product (transpose not necessary here since they are 1-dim vectors)\n",
        "\n",
        "print(attn_scores_2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sX-mLYZzz1NB",
        "outputId": "f34435b9-a2cd-4329-d5b6-2b0d61750a25"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "How to calculate dot product:\n",
        "\n",
        "dot(a,b) = a1xb1 + a2xb2 + a3xb3 + .... + anxbn\n",
        "\n",
        "for example (using our earlier input vector):\n",
        "\n",
        "dot(input_x1,query_x2) = dot([0.43, 0.15, 0.89],[0.55, 0.87, 0.66])\n",
        "\n",
        "                        = (0.43x0.55)+(0.15x0.87)+(0.89x0.66)\n",
        "                        = 0.9544\n"
      ],
      "metadata": {
        "id": "Kgtcl4EH5VyW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "res = 0.\n",
        "\n",
        "for idx, element in enumerate(inputs[0]):\n",
        "    res += inputs[0][idx] * query[idx]\n",
        "\n",
        "print(res)\n",
        "print(torch.dot(inputs[0], query))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yltPZHYaz1g1",
        "outputId": "db2ce5d9-5f0c-4ccf-f42b-4f8614b6a04f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.9544)\n",
            "tensor(0.9544)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dot product is a measure of similarity because it quantifies how closely two vectors are aligned: a higher dot product indicates a greater degree of alignment or similarity between the vectors. In the context of self-attention mechanisms, the dot product determines the extent to which each element in a sequence focuses on, or \"attends to,\" any other element: the higher the dot product, the higher the similarity and attention score between two elements."
      ],
      "metadata": {
        "id": "BNg6TvFV6ZLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the next step, we normalize each of the attention scores we computed previously. The main goal behind the normalization is to obtain the attention weights that sum up to 1. This normalization is a convention that is useful for interpretation and maintaning training stability in an LLM."
      ],
      "metadata": {
        "id": "3-rRDXVs7WHX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "attn_weights_2_tmp = attn_scores_2 / attn_scores_2.sum()\n",
        "\n",
        "print(\"Attention weights:\", attn_weights_2_tmp)\n",
        "print(\"Sum:\", attn_weights_2_tmp.sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gl_TJNlsz2i-",
        "outputId": "c47634f2-ce17-4ad8-a526-9d5001dfd29e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention weights: tensor([0.1455, 0.2278, 0.2249, 0.1285, 0.1077, 0.1656])\n",
            "Sum: tensor(1.0000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the output, the attention weights now sum to 1\n",
        "<img src=\"https://drive.google.com/uc?id=1jvu08NA_G2vb1r9SHeyKMP59UkZoiNei\">"
      ],
      "metadata": {
        "id": "PFJpB3Q7_72L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In practice, it's more common and advisable to use the softmax function for normalization. This approach is better at managing extreme values and offers more favorable gradient properties during training. The following is a basic implementation of the softmax function for normalizing the attention scores:"
      ],
      "metadata": {
        "id": "TR5FjmnwBzuv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax_naive(x):\n",
        "    return torch.exp(x) / torch.exp(x).sum(dim=0)\n",
        "\n",
        "attn_weights_2_naive = softmax_naive(attn_scores_2)\n",
        "\n",
        "print(\"Attention weights:\", attn_weights_2_naive)\n",
        "print(\"Sum:\", attn_weights_2_naive.sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o0TXz5zkz43S",
        "outputId": "143723a6-312f-43a9-d9db-c6772935b39b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention weights: tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
            "Sum: tensor(1.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As the output shows, the softmax function also meets the objective and normalizes the attention weights such that they sum to 1"
      ],
      "metadata": {
        "id": "KEcuu9QbCI-4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In addition, the softmax function ensures that the attention weights are always positive. This make the output interpretable as probabilities or relative importance, where higher weights indicate greater importance.\n",
        "\n",
        "Note that this naive softmax implementation (softmax_naive) may encounter numerical instability problems, such as overflow and underflow, when dealing with large or small input values. Therefore, in practice, it's advisable to use the PyTorch implementation of softmax, which has been extensively optimized for performance:"
      ],
      "metadata": {
        "id": "hETACGIbCgxd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "attn_weights_2 = torch.softmax(attn_scores_2, dim=0)\n",
        "\n",
        "print(\"Attention weights:\", attn_weights_2)\n",
        "print(\"Sum:\", attn_weights_2.sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "StTrmj8qz6Zy",
        "outputId": "3635e85a-25bc-42cc-c901-27834118bf18"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention weights: tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
            "Sum: tensor(1.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After computing the normalized attention weights, the last step is to calculate the context vector z2 by multiplying the embedded input tokens, xi, with the corresponding attention weights and then summing the resulting vectors. Thus, the context vector z2 is the weighted sum of all input vectors, obtained by multiplying each input vector by its corresponding attention weight:"
      ],
      "metadata": {
        "id": "lMUQbqnDDDJw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = inputs[1] # 2nd input token is the query\n",
        "context_vec_2 = torch.zeros(query.shape)\n",
        "for i,x_i in enumerate(inputs):\n",
        "    print(\"attention weights: {} | embedded input token: {}\".format(attn_weights_2[i], x_i))\n",
        "    context_vec_2 += attn_weights_2[i]*x_i\n",
        "\n",
        "print(context_vec_2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fAVNBm4Hz7kt",
        "outputId": "15260d8c-8df0-40d8-ab5c-445a076f50ea"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "attention weights: 0.13854756951332092 | embedded input token: tensor([0.4300, 0.1500, 0.8900])\n",
            "attention weights: 0.2378913015127182 | embedded input token: tensor([0.5500, 0.8700, 0.6600])\n",
            "attention weights: 0.23327402770519257 | embedded input token: tensor([0.5700, 0.8500, 0.6400])\n",
            "attention weights: 0.12399158626794815 | embedded input token: tensor([0.2200, 0.5800, 0.3300])\n",
            "attention weights: 0.10818186402320862 | embedded input token: tensor([0.7700, 0.2500, 0.1000])\n",
            "attention weights: 0.15811361372470856 | embedded input token: tensor([0.0500, 0.8000, 0.5500])\n",
            "tensor([0.4419, 0.6515, 0.5683])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://drive.google.com/uc?id=1dwjwcsif32LI_glFql5zSkLWYKhJP0D4\">\n",
        "\n",
        "Next, we will generalize the procedure for computing context vectors to calculate all context vectors simultaneously"
      ],
      "metadata": {
        "id": "Z1jPJ7ewG2WJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3.3.2 Computing Attention Weights for All Input Tokens**"
      ],
      "metadata": {
        "id": "_sUdKx4oPkY1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "So far, we have computed attention weights and the context vector for input 2, as shown in the highlighted row in below figure. Now let's extend this computation to calculate attention weights and context vectors for all inputs\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1LNedYJwe7f5b3sRMVdkINtCffWb55Msa\">\n",
        "\n",
        "We follow the same three steps as before, except that we make a few modifications in the code to compute all context vectors instead of only the second one, z2"
      ],
      "metadata": {
        "id": "jD4Pzcu-P4u0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "attn_scores = torch.empty(6, 6)\n",
        "\n",
        "for i, x_i in enumerate(inputs):\n",
        "    for j, x_j in enumerate(inputs):\n",
        "        attn_scores[i, j] = torch.dot(x_i, x_j)\n",
        "\n",
        "print(attn_scores)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L1QSXnE6UkTV",
        "outputId": "7c0f6c0a-3249-4685-b0c6-e1204e7597cc"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
            "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
            "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
            "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
            "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
            "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://drive.google.com/uc?id=1QEHg5PWEmc_bBkojiWHGDY2xKvEJLvho\">"
      ],
      "metadata": {
        "id": "3Slj_ZB4VuSt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each element in the tensor represents an attention score between each pair of inputs.\n",
        "\n",
        "When computing the preceding attention score tensor, we used for loops in Python. However, for loops are generally slow, and we can achieve the same results using matrix multiplication:"
      ],
      "metadata": {
        "id": "TPjBBC0BV4h1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "attn_scores = inputs @ inputs.T\n",
        "print(attn_scores)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lx1LAEV8UobE",
        "outputId": "3cac06d1-b7f0-4005-c210-d26f23376103"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
            "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
            "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
            "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
            "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
            "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, let's normalize these attention scores using softmax function so the value of each row is sum to 1:"
      ],
      "metadata": {
        "id": "s8LxXXnbWnGy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "attn_weights = torch.softmax(attn_scores, dim=-1)\n",
        "print(attn_weights)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_5-ZLhcmUqFM",
        "outputId": "1d5b2c45-9791-4567-8d85-4ed2abc755e5"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.2098, 0.2006, 0.1981, 0.1242, 0.1220, 0.1452],\n",
            "        [0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581],\n",
            "        [0.1390, 0.2369, 0.2326, 0.1242, 0.1108, 0.1565],\n",
            "        [0.1435, 0.2074, 0.2046, 0.1462, 0.1263, 0.1720],\n",
            "        [0.1526, 0.1958, 0.1975, 0.1367, 0.1879, 0.1295],\n",
            "        [0.1385, 0.2184, 0.2128, 0.1420, 0.0988, 0.1896]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the context of using PyTorch, the dim parameter in functions like torch.softmax specifies the dimension of the input tensor along which the function will be computed. By setting dim=-1, we are instructing the softmax function to apply the normalization along the last dimension of the attn_scores tensor. If attn_scores is a two-dimensional tensor (for example, with a shape of [rows, columns]), it will normalize accross the columns so that the values in each row (summing over the column dimension) sum up to 1.\n",
        "\n",
        "We can verify that the rows indeed all sum to 1:"
      ],
      "metadata": {
        "id": "2Ce4Sy8lW6Iu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "row_2_sum = sum([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
        "print(\"Row 2 sum:\", row_2_sum)\n",
        "\n",
        "print(\"All row sums:\", attn_weights.sum(dim=-1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wS0mUJ86Ur2d",
        "outputId": "cf61138c-2f8d-4c3a-b687-b1a69da5fa52"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Row 2 sum: 1.0\n",
            "All row sums: tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Last step, we use these attention weights to calculate the context vector via matrix multiplication:"
      ],
      "metadata": {
        "id": "vm2BTFcIXqdt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_context_vecs = attn_weights @ inputs\n",
        "print(all_context_vecs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ExJlWLzgUuE7",
        "outputId": "622137bb-fe0e-4041-f877-9712ac1b379c"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.4421, 0.5931, 0.5790],\n",
            "        [0.4419, 0.6515, 0.5683],\n",
            "        [0.4431, 0.6496, 0.5671],\n",
            "        [0.4304, 0.6298, 0.5510],\n",
            "        [0.4671, 0.5910, 0.5266],\n",
            "        [0.4177, 0.6503, 0.5645]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can double-check that the code is correct by comparing the second row with the context vector z2 that we computed in section 3.3.1:"
      ],
      "metadata": {
        "id": "NcWZ72drYDwp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Previous 2nd context vector:\", context_vec_2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0hEiLjp5UvoN",
        "outputId": "9b737180-6ac6-4d68-a0d7-8a6d9cdac4f7"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Previous 2nd context vector: tensor([0.4419, 0.6515, 0.5683])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on the result, we can see the exact same result for context vector z2.\n",
        "\n",
        "This concludes the code walkthrough of a simple self-attention mechanism. Next, we will add trainable weights, enabling the LLM to learn from data and improve its performance on specific tasks."
      ],
      "metadata": {
        "id": "xSqVgfYiYNng"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3.4 Implementing Self-Attention with Trainable Weights**"
      ],
      "metadata": {
        "id": "JhPQK5O8YfBa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our next step will be to implement the self-attention mechanism used in the original transformer architecture, the GPT models, and most other popular LLMs. This self-attention mechanism is also called scaled dot-product attention. Figure below shows how this self-attention mechanism fits into broader context of implementing an LLM.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1Ii-dv8sAL3SEQXFvCG95CZiCXjMBCyhY\">\n"
      ],
      "metadata": {
        "id": "uUDSYyjQ9lHL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The self-attention mechanism with trainable weights build on the previous concepts: we want to compute context vectors as weighted sums over the input vectors specific to a certain input element. There are only slight difference compared to our simplified self-attention earlier.\n",
        "\n",
        "The most notable difference is the introduction of weight matrices that are updated during model training. These trainable weight matrices are crucial so that the model (specifically, the attention module inside the model) can learn to produce \"good\" context vectors.\n",
        "\n",
        "We will divide the self-attention mechanism into 2 parts.\n",
        "* First, we will code it step by step as before.\n",
        "* Second, we will organize the code into a compact Python class that can be imported into the LLM architecture."
      ],
      "metadata": {
        "id": "br9FBlk9_ZSK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3.4.1 Computing the Attention Weights Step by Step**"
      ],
      "metadata": {
        "id": "hSS3FIzQAeHq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will implement the self-attention mechanism step by step by introducing the three trainable weight matrices Wq, Wk, and Wv. There matrices are used to project the embedded input tokens, xi, into query, key, and value vectors, respectively, as illustrated in figure below\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1KVfx4qwvYCe9mAC6mj3Xp_yK_STyhGwE\">"
      ],
      "metadata": {
        "id": "sqqSqySVAjCR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Earlier, we defined the second input element x2 as the query when we computed the simplified attention weights to compute the context vector z2. Then we generalized this to compute all context vectors z1 ... zT for the six-word input sentence. \"Your journey starts with one step.\"\n",
        "\n",
        "Similarly, we start here by computing only one context vector, z2, for illustration purposes. We will then modify this code to calculate all context vectors.\n",
        "\n",
        "Let's begin by defining a few variables:"
      ],
      "metadata": {
        "id": "TuaUZnysDiij"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_2 = inputs[1] # second input element\n",
        "d_in = inputs.shape[1] # the input embedding size, d=3\n",
        "d_out = 2 # the output embedding size, d=2"
      ],
      "metadata": {
        "id": "S7wm6JpcHsFY"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the GPT-like models, the input and output dimensions are usually the same, but to better follow the computation, we'll use different input (d_in=3) and output (d_out=2) dimensions here.\n",
        "\n",
        "Next, we intialize the three weight matrices Wq, Wk, and Wv."
      ],
      "metadata": {
        "id": "0hhtjlaBIB_n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "\n",
        "W_query = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
        "W_key   = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
        "W_value = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
        "\n",
        "print(\"W_query: {}\".format(W_query))\n",
        "print(\"W_key: {}\".format(W_key))\n",
        "print(\"W_value: {}\".format(W_value))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IH1xmvG_HxZO",
        "outputId": "4dd2d719-0dd3-4759-865b-6759923b6fa5"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W_query: Parameter containing:\n",
            "tensor([[0.2961, 0.5166],\n",
            "        [0.2517, 0.6886],\n",
            "        [0.0740, 0.8665]])\n",
            "W_key: Parameter containing:\n",
            "tensor([[0.1366, 0.1025],\n",
            "        [0.1841, 0.7264],\n",
            "        [0.3153, 0.6871]])\n",
            "W_value: Parameter containing:\n",
            "tensor([[0.0756, 0.1966],\n",
            "        [0.3164, 0.4017],\n",
            "        [0.1186, 0.8274]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We set requires_grad=False to reduce clutter in the outputs, but if we were to use the weight matrices for model training, we would set requires_grad=True to update these matrices during model training.\n",
        "\n",
        "Next, we compute the query, key, and value vectors:"
      ],
      "metadata": {
        "id": "WAoUsNLqI3_-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query_2 = x_2 @ W_query # _2 because it's with respect to the 2nd input element\n",
        "key_2 = x_2 @ W_key\n",
        "value_2 = x_2 @ W_value\n",
        "\n",
        "print(query_2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bnhOFzHIHyFH",
        "outputId": "6000cb88-ae09-455a-ec4b-0a7bf2c054f9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.4306, 1.4551])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The output for the query results in a two-dimensional vector since we set the number of columns of the corresponding weight matrix, via d_out, to 2."
      ],
      "metadata": {
        "id": "rdOzaoN1KBzt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Weight parameters vs. attention weights**\n",
        "\n",
        "In the weight matrices W, the term \"weight\" is short for \"weight parameters,\" the values of a neural network that are optimized during training. This is not to be confused with the attention weights. Attention weights determine the extent to which a context vector depends on the different parts of the input.\n",
        "\n",
        "In summary, weight parameters are the fundamental, learned coefficients that define the network's connections, while attention weights are dynamic, context-specific values."
      ],
      "metadata": {
        "id": "-jtmcnKaKPsc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Even though our temporary goal is only compute the one context vector, z2, we still require the key and value vectors for all input elements as they are involved in computing the attention weights with respect to the query q2.\n",
        "\n",
        "We can obtain all keys and values via matrix multiplication:"
      ],
      "metadata": {
        "id": "6S8LSmoWLAtH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "keys = inputs @ W_key\n",
        "values = inputs @ W_value\n",
        "\n",
        "print(\"keys.shape:\", keys.shape)\n",
        "print(\"values.shape:\", values.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1BGVgxvrHziP",
        "outputId": "0e4bf89b-6c5c-46fd-b812-2026f3b8b798"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "keys.shape: torch.Size([6, 2])\n",
            "values.shape: torch.Size([6, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We successfully projected the six input tokens from a three-dimensional onto a two-dimensional embedding space"
      ],
      "metadata": {
        "id": "y37X_vysLb8-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The second step is to compute the attention scores:\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1pQN0FaxIuyl_KCeR54jx9c-CDi0qksyd\">\n",
        "The attention score computation is a dot-product computation similar to what we used in the simplified self-attention mechanism in section 3.3. The new aspect here is that we are not directly computing the dot-product between the input elements but using the query and key obtained by transforming the inputs via the respective weight matrices.\n",
        "\n",
        "First, let's compute the attention score ω22:"
      ],
      "metadata": {
        "id": "ge0HrIZ1NmRQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "keys_2 = keys[1] # Python starts index at 0\n",
        "attn_score_22 = query_2.dot(keys_2)\n",
        "print(attn_score_22)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wnwew0tXH0du",
        "outputId": "4307e1b1-8bac-4787-ff75-4e709e7ab857"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(1.8524)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Again, we can generalize this computation to all attention scores via matrix multiplication:"
      ],
      "metadata": {
        "id": "dYhhF9mjRKnE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "attn_scores_2 = query_2 @ keys.T # All attention scores for given query\n",
        "print(attn_scores_2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OM5FVGNvH1vB",
        "outputId": "f0a6860c-0760-45f5-96be-e0bf296d36f7"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1.2705, 1.8524, 1.8111, 1.0795, 0.5577, 1.5440])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we get the attention scores, we need to calculate the attention weight. We compute the attention weights by scaling the attention scores and using the softmax function. However, now we scale the attention scores by dividing them by the square root of the embedding dimension of the keys (taking the square root is mathematically the same as exponentiating by 0.5)\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1XeZfgHyyNZbAu9-oWRmN727g8q0Hy5gE\">"
      ],
      "metadata": {
        "id": "kRqCctDBRu0y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "d_k = keys.shape[1]\n",
        "print(d_k)\n",
        "attn_weights_2 = torch.softmax(attn_scores_2 / d_k**0.5, dim=-1)\n",
        "print(attn_weights_2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z4YNisbqH244",
        "outputId": "2464a925-ebf0-4d55-b8b6-113a72b55d94"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n",
            "tensor([0.1500, 0.2264, 0.2199, 0.1311, 0.0906, 0.1820])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The rationale behind scaled-dot product attention**\n",
        "\n",
        "The reason for the normalization by the embedding dimension size is to improve the training performance by avoiding small gradients. For instance, when scaling up the embedding dimension, which is typically greater than 1000 for GPT-like LLMs, large dot products can result in very small gradients during backpropagation due to the softmax function applied to them. As dot products increase, the softmax function behaves more like a step function, resulting in gradients nearing zero. These small gradients can drastically slow down learning or cause training to stagnate.\n",
        "\n",
        "The scaling by the square root of the embedding dimension is the reason why this self-attention mechanism is also called scaled-dot product attention"
      ],
      "metadata": {
        "id": "BpyucxPuUNZm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, the final step is to compute the context vectors:\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1FlP3au_su_2eX_to_uAyPCFlSi0Y-18_\">"
      ],
      "metadata": {
        "id": "ZR_e01nJU6fZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Similar to when we computed the context vector as a weighted sum over the input vectors, we now compute the context vector as a weighted sum over the value vectors. Here, the attention weights serve as a weighting factor that weighs the respective importance of each value vector. Also as before, we can use matrix multiplication:"
      ],
      "metadata": {
        "id": "OfBe8flxXi97"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "context_vec_2 = attn_weights_2 @ values\n",
        "print(context_vec_2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pF9X_Pm6H4A6",
        "outputId": "99b634eb-2f5e-4f89-a40e-d3f0842f0ae5"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.3061, 0.8210])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "So far, we've only computed a single context vector, z2. Next, we will generalize the code to compute all context vectors in the input sequence, z1 to zT."
      ],
      "metadata": {
        "id": "iJ6QTJY0X7Za"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Why query, key, and value?**\n",
        "\n",
        "The terms \"key\", \"query\", and \"value\" in the context of the attention mechanisms are borrowed from the domain of information retrieval and databases, where similar concepts are used to store, search, and retrieve information.\n",
        "\n",
        "A query is analogous to a search query in a database. It represents the current item (e.g. a word or token in a sentence) the model focuses on or tries to understand. The query is used to probe the other parts of the input sequence to determine how much attention to pay to them.\n",
        "\n",
        "The key is like a database key used for indexing and searching. In the attention mechanism, each item in the input sequence (e.g., each word in a sentence) has an associated key. These keys are used to match the query.\n",
        "\n",
        "The value in this context is similar to the value in a key-value pair in a database. It represents the actual content or representation of the input items. Once the model determines which keys (and thus which parts of the input) are most relevant to the query (the current focus item), it retrieves the corresponding values.\n"
      ],
      "metadata": {
        "id": "H700Kg7hYJDo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3.4.2 Implementing a Compact Self-Attention Python Class**"
      ],
      "metadata": {
        "id": "-BSwz8KwZbVJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we need to implement all we learned before into a Python class"
      ],
      "metadata": {
        "id": "d_o0WudyZ0YT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class SelfAttention_v1(nn.Module):\n",
        "\n",
        "    def __init__(self, d_in, d_out):\n",
        "        super().__init__()\n",
        "        self.W_query = nn.Parameter(torch.rand(d_in, d_out))\n",
        "        self.W_key   = nn.Parameter(torch.rand(d_in, d_out))\n",
        "        self.W_value = nn.Parameter(torch.rand(d_in, d_out))\n",
        "\n",
        "    def forward(self, x):\n",
        "        keys = x @ self.W_key\n",
        "        queries = x @ self.W_query\n",
        "        values = x @ self.W_value\n",
        "\n",
        "        attn_scores = queries @ keys.T # omega\n",
        "        attn_weights = torch.softmax(\n",
        "            attn_scores / keys.shape[-1]**0.5, dim=-1\n",
        "        )\n",
        "\n",
        "        context_vec = attn_weights @ values\n",
        "        return context_vec"
      ],
      "metadata": {
        "id": "t9iVXyb6Zn6c"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this PyTorch code, SelfAttention_v1 is a class derived from nn.Module, which is a fundamental building block of PyTorch models that provides necessary functionalities for model layer creation and management.\n",
        "\n",
        "The __init__ method initializes trainable weights matrices (W_query, W_key, and W_value) for queries, keys, and values, each transforming the input dimension d_in to an output dimension d_out.\n",
        "\n",
        "During the forward pass, using the forward method, we compute attention scores (attn_scores) by multiplying queries and keys, normalizing these scores using softmax. Finally, we create a context vector by weighting the values with these normalized attention scores.\n",
        "\n",
        "This is how to implement the class:"
      ],
      "metadata": {
        "id": "EQNNREaGaES6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "sa_v1 = SelfAttention_v1(d_in, d_out)\n",
        "print(sa_v1(inputs))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BVtDEDOmbBNZ",
        "outputId": "ca008e9e-8920-4161-c8b4-137d4596af30"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.2996, 0.8053],\n",
            "        [0.3061, 0.8210],\n",
            "        [0.3058, 0.8203],\n",
            "        [0.2948, 0.7939],\n",
            "        [0.2927, 0.7891],\n",
            "        [0.2990, 0.8040]], grad_fn=<MmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since inputs contains six embedding vectors, this results in a matrix storing the six context vectors.\n",
        "\n",
        "We can improve the SelfAttention_v1 implementation further by utilizing PyTorch's nn.Linear layers, which effectively perform matrix multiplication when the bias units are disabled. Additional advantage is instead of manually implementing nn.Parameter(torch.rand(...)) is that nn.Linear has an optimized weight initialization scheme, contributing to more stable and effective model training\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1M2BCfz7LSWmp_TtG4VGU9MZrTDN-pYkF\">"
      ],
      "metadata": {
        "id": "8-TH9E48bJ-d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SelfAttention_v2(nn.Module):\n",
        "\n",
        "    def __init__(self, d_in, d_out, qkv_bias=False):\n",
        "        super().__init__()\n",
        "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        keys = self.W_key(x)\n",
        "        queries = self.W_query(x)\n",
        "        values = self.W_value(x)\n",
        "\n",
        "        attn_scores = queries @ keys.T\n",
        "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
        "\n",
        "        context_vec = attn_weights @ values\n",
        "        return context_vec\n",
        "\n",
        "torch.manual_seed(789)\n",
        "sa_v2 = SelfAttention_v2(d_in, d_out)\n",
        "print(sa_v2(inputs))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4hP6pTlRZruF",
        "outputId": "b3c15c63-f221-45bb-a5d9-b38ec26cc18b"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-0.0739,  0.0713],\n",
            "        [-0.0748,  0.0703],\n",
            "        [-0.0749,  0.0702],\n",
            "        [-0.0760,  0.0685],\n",
            "        [-0.0763,  0.0679],\n",
            "        [-0.0754,  0.0693]], grad_fn=<MmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We only change the init part where we intialize the weight of query, key, and value.\n",
        "\n"
      ],
      "metadata": {
        "id": "og8qv4sGg21a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we will make enhancements to the self-attention mechanism, focusing specifically on incorporating causal and multi-head elements. The causal aspect involves modifying the attention mechanism to prevent the model from accessing the future information in the sequence, which is crucial for tasks like language modeling, where each word prediction should only depend on previous words.\n",
        "\n",
        "The multi-head component involves splitting the attention mechanism into multiple \"heads.\" Each head learns different aspects of the data, allowing the model to simultaneously attend to information from different representation subspaces at different positions. This improves the model's performance in complex tasks."
      ],
      "metadata": {
        "id": "R7HA1ZdQhQ1L"
      }
    }
  ]
}